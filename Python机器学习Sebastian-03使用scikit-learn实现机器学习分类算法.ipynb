{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本章中，我们将学到如下内容：\n",
    "+ 介绍常用分类算法的概念\n",
    "+ 使用scikit-learn机器学习库\n",
    "+ 选择机器学习算法时需要注意的问题  \n",
    "\n",
    "5  \n",
    "# 3.1 分类算法的选择\n",
    "再次借用一下“没有免费午餐”理论：没有任何一种分类器可以在所有可能的应用场景下都有良好的表现  \n",
    "总而言之，分类器的性能、计算能力和预测能力，在很大程度上都依赖于用于模型训练的相关数据。训练机器学习算法所涉及的五个主要步骤可以概述如下：  \n",
    "1. 特征的选择  \n",
    "2. 确定性能评价标准\n",
    "3. 选择分类器及其优化算法\n",
    "4. 对模型性能的评估\n",
    "5. 算法的调优\n",
    "5  \n",
    "\n",
    "# 3.2 初涉scikit-learn的使用\n",
    "## 使用sckit-learn训练感知器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tdatasets\tload_iris() feature_names\n",
    "python\tsklearn\tdatasets\tload_iris() target\n",
    "python\tsklearn\tdatasets\tload_iris() data\n",
    "'''\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target\n",
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了评估训练得到的模型在未知数据上的表现，我们进一步将数据集划分为训练数据集和测试数据集 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tmodel_selection\ttrain_test_split()\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此，我们将使用scikit-learn的preprocessing模块中的standardscaler类对特征进行标准化处理  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler() \n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的是，我们要使用相同的缩放参数分别处理训练和测试数据集，以保证它们的值是彼此相当的  \n",
    "在对训练数据做了标准化处理后，我们现在可以训练感知器模型了。scikit-learn中的大多数算法本身就使用一对多方法来支持多类别分类，因此，我们可以将三种鸢尾花的数据同时输入到感知器中    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tLinear Models\tPerceptron() max_iter\n",
    "python\tsklearn\tLinear Models\tPerceptron() eta0\n",
    "'''\n",
    "from sklearn.linear_model import Perceptron\n",
    "ppn = Perceptron(max_iter = 40, eta0 = 0.1, random_state = 0)\n",
    "ppn.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此模型的参数eta0与我们自行实现的感知器中的学习速率eta等价，而参数max_iter定义了迭代的次数  \n",
    "与第2章中实现的感知器一样，使用scikit-learn完成模型的训练后，就可以在测试数据集上使用predict方法进行预测了 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tstr\tformat()\tformat() d\n",
    "'''\n",
    "y_pred = ppn.predict(X_test_std)\n",
    "print('Misclassified samples: {:d}'.format((y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "许多机器学习从业者通常使用准确率而不是误分类来评判一个模型，它们关系如下：  \n",
    "1 - 误分类率 = 准确率  \n",
    "可以通过metrics模块计算感知器在测试数据集上的分类准确率：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tClassification metrics\taccuracy_score()\n",
    "'''\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制刚刚训练过的模型的决策区域，我们做少许修改：使用小圆圈来高亮显示来自测试数据集的样本 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tArray manipulation routines\tunique()\n",
    "python\tmatplotlib\tcolors\tListedColormap()\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\tmin()\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\tmax()\n",
    "python\tnumpy\tArray creation routines\tmeshgrid()\n",
    "python\tmatplotlib\tPyplot function overview\tcontourf()\n",
    "python\tmatplotlib\tPyplot function overview\tscatter() alpha\n",
    "python\tmatplotlib\tcollections\tlabel\n",
    "'''\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_decision_regions(X, y, classifier,\n",
    "                          test_idx = None, resolution = 0.02):\n",
    "    # 设置散点生成器和色图\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    \n",
    "    # 绘制决策边界\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                          np.arange(x2_min, x2_max, resolution)) \n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha = 0.4, cmap = cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    \n",
    "    # 绘制样本分类\n",
    "    X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x = X[y == cl, 0], y = X[y == cl, 1],\n",
    "                   alpha = 0.8, color = cmap(idx),\n",
    "                   marker = markers[idx], label = cl)\n",
    "\n",
    "    # 高亮测试样本\n",
    "    if test_idx:\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1], c = 'yellow',\n",
    "                   alpha = 0.5, linewidth = 1, marker = 'o',\n",
    "                   s = 55, label = 'test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tArray manipulation routines\tvstack()\n",
    "python\tnumpy\tArray manipulation routines\thstack()\n",
    "python\tmatplotlib\tPyplot function overview\tlegend() loc\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "X_combined_std = np.vstack((X_train_std, X_test_std))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "plot_decision_regions(X = X_combined_std,\n",
    "                     y = y_combined,\n",
    "                     classifier = ppn,\n",
    "                     test_idx = range(105,150))\n",
    "plt.xlabel('petal length [standardized]')\n",
    "plt.ylabel('petal length [standardized]')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从结果呈现的图像中我们可以看到，无法通过一个线性决策边界完美区分三类样本  \n",
    "对于无法完美线性可分的数据集，感知器算法将永远无法收敛，这也是在实践中一般不推荐使用感知器算法的原因 5\n",
    "# 3.3 逻辑斯蒂回归中的类别概率\n",
    "感知器在样本不是完全线性可分的情况下不会收敛的原因：在每次迭代过程中，总是存在至少一个分类错误的样本，从而导致额权重持续更新  \n",
    "为了提高分类的效率，我们学习另外一种针对线性二类别分类问题的简单但更高效的算法：逻辑斯蒂回归 5  \n",
    "## 3.3.1 初识逻辑斯蒂回归与条件概率\n",
    "与感知器及Adaline类似，逻辑斯蒂回归模型也是适用于二类别分类问题的线性模型，通过一对多技术可以扩展到多类别分类  \n",
    "我们先介绍一下几率比，它指的是特定事件发生的几率。用数学公式表示为$\\frac{p}{(1-p)}$，其中p为正事件发生的概率  \n",
    "正事件是指我们所要预测的事件，以一个患者患有某种疾病的概率为例，我们可以将正事件的类标标记为y=1。更进一步，我们可以定义logit函数，它是几率比的对数函数(log-odds, 对数几率)  \n",
    "$$logit(p)=log\\frac{p}{(1-p)}$$\n",
    "logit函数的输入值范围介于区间[0,1]，它能将输入转换到整个实数范围内，由此可以将度数几率记为输入特征值的线性表达式 5  \n",
    "$$logit(p(y=1|x))=w_0x_0+w_1x_1+\\dots+w_mx_m=\\sum_{i=0}^n{w_mx_m}=w^Tx$$\n",
    "此处，$p(y=1|x)$是在给定特征x的条件下，某一个样本属于类别1的条件概率  \n",
    "我们在此的真正目的是预测某一样本属于特定类别的概率，它是logit函数的反函数，也称作logistic函数，由于它的图像呈S形，因此有时也简称sigmoid函数  \n",
    "$$\\phi(z)=\\frac{1}{1+e^{-z}}$$\n",
    "这里的z为净输入，也就是样本特征与权重的线性组合，其计算方式为： 5  \n",
    "$$z=w^Tx=w_0x_0+w_1x_1+\\dots+w_mx_m$$\n",
    "我们来绘制一下自变量取值介于区间[-7,7]的sigmoid函数的图像  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tMathematical functions\texp()\n",
    "python\tmatplotlib\tPyplot function overview\taxvline()\n",
    "python\tmatplotlib\tlines\tcolor\n",
    "python\tmatplotlib\tPyplot function overview\taxhspan()\n",
    "python\tmatplotlib\tpatches\tPatch() facecolor\n",
    "python\tmatplotlib\tpatches\tPatch() alpha\n",
    "python\tmatplotlib\tpatches\tPatch() linestyle\n",
    "python\tmatplotlib\tPyplot function overview\taxhline()\n",
    "python\tmatplotlib\tlines\tlinestyle\n",
    "\n",
    "'''\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "z = np.arange(-7, 7, 0.1)\n",
    "phi_z = sigmoid(z)\n",
    "plt.plot(z, phi_z)\n",
    "plt.axvline(0.0, color='k')\n",
    "plt.axhspan(0.0, 1.0, facecolor='1.0', alpha=1.0, linestyle='dotted')\n",
    "plt.axhline(y=0.5, linestyle='dotted', color='black')\n",
    "plt.axhline(y=0, linestyle='dotted', color='black')\n",
    "plt.axhline(y=1.0, linestyle='dotted', color='black')\n",
    "plt.yticks([0.0, 0.5, 1.0])\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('$\\phi (z)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行上述代码，将会呈现一个S形曲线  \n",
    "可以看到，当z趋向于无穷大时($z\\to\\infty$)时，$\\varphi(z)$趋近于1，这是由于$e^{-z}$在z值极大的情况下其值变得极小。类似的，反之亦然 5  \n",
    "由此，可以得出结论：sigmoid函数以实数值作为输入并将其映射到[0,1]区间，其拐点位于$\\varphi(z)=0.5$处  \n",
    "在Adaline中，我们使用恒等函数$\\varphi(z)=z$作为激励函数。而在逻辑斯蒂回归中，只是简单地将前面提及的sigmoid函数作为激励函数，如下图所示：  \n",
    "![3-1](../syn_pic/py_machine_learning/3-1.png)\n",
    "在给定特征x及其权重w的情况下，sigmoid函数的输出给出了特定样本x属于类别1的概率$\\varphi(z)=P(y=1|x;w)$  \n",
    "预测得到的概率可以通过一个量化器(单位阶跃函数)简单地转换为二元输出： 5  \n",
    "$$\\hat{y}=\\begin{cases}1&若\\phi(z)\\ge0.5 \\\\ 0&其他\\end{cases}$$\n",
    "对照前面的sigmoid函数的图像，它其实相当于：\n",
    "$$\\hat{y}=\\begin{cases}1&若z\\ge0.0 \\\\ 0&其他\\end{cases}$$\n",
    "对于许多应用实践来说，我们不但对类标预测感兴趣，而且对事件属于某一类别的概率进行预测也非常有用 5  \n",
    "## 3.3.2 通过逻辑斯蒂回归模型的代价函数获得权重  \n",
    "在上一章中，我们将其代价函数定义为误差平方和(SSE)  \n",
    "$$J(w)=\\sum_i{\\frac{1}{2}(\\phi(z^{(i)}-y^{(i)}))^2}$$\n",
    "通过最小化此代价函数，我们可以得到Adaline分类模型的权重w。为了推导出逻辑斯蒂回归模型的代价函数，我们需要先定义一个最大似然函数L，假定数据集中的每个样本都是相互独立的，其计算公式如下：    \n",
    "$$L(w)=P(y|x;w)=\\prod_{i=1}^n{P(y^{(i)}|x^{(i)};w)}=(\\phi(z^{(i)}))^{y^{(i)}}(1-\\phi(z^{(i)}))^{1-y^{(i)}} $$\n",
    "在实际应用中，很容易对此方程的自然对数进行最大化处理，故定义了对数似然函数： 5  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
