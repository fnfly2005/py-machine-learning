{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本章简介\n",
    "另一种常用于降维方法就是特征抽取，特征抽取可以将原始数据集变化到一个维度更低的新的特征子空间，从而对数据进行压缩，避免维度灾难  \n",
    "本章读者将学习三种降维技术：  \n",
    "+ 无监督数据压缩——主成分分析(PCA)\n",
    "+ 基于类别可分最大化的监督降维技术——线性判别分析(LDA)\n",
    "+ 通过核主成分分析进行非线性降维 5  \n",
    "\n",
    "# 5.1 无监督数据降维技术-主成分分析\n",
    "> 本章介绍了主成分分析(PCA)的原理、实现和使用方法\n",
    "\n",
    "### 问题：特征选择和特征抽取有什么区别？\n",
    "> 特征抽取后的新特征是原来特征的一个映射，而特征选择后的特征是原来特征的一个子集 5  \n",
    "\n",
    "特征抽取算法会将数据转换或者映射到一个新的特征空间，可以理解为：在尽可能保持相关信息的情况下，对数据进行压缩的一种方法  \n",
    "特征抽取通常用于提高计算效率，同样也可以帮助我们降低\"维度灾难\"——尤其当模型不适于正则化处理时  \n",
    "主成分分析(PCA)是一种广泛应用于不同领域的无监督线性数据转换技术，其突出作用是降维  \n",
    "简而言之，PCA的目标是在高维数据中找到最大方差的方向，并将数据映射到一个维度不大于原始数据的新的子空间上 5  \n",
    "如下图所示，新的子空间上正交的坐标轴（主成分）可被解释为方差最大的方向。在此，$x_1$和$x_2$为原始特征的坐标轴，而PC1和PC2即为主成分  \n",
    "![5-1](../syn_pic/py_machine_learning/5-1.png)\n",
    "如果使用PCA降维，我们将构建一个$d\\times k$维的转换矩阵W，这样就可以将一个样本向量x映射到一个新的k维特征子空间上去，此空间的维度小于原始的d维特征空间：  \n",
    "$$x=[x_1,x_2,\\dots,x_d]，x\\in{R}^d$$\n",
    "$$\\downarrow xW,W\\in{R}^{d\\times k}$$\n",
    "$$z=[z_1,z_2,\\dots,z_k],z\\in{R}^k$$\n",
    "完成从原始d维数据到新的k维子空间(一般情况下$k\\ll d$)的转换后，第一主成分的方差应该是最大的，由于各主成分之间是不相关的（正交的），后续各主成分也具备尽可能大的方差  \n",
    "需要注意的是，主成分的方向对数据值的范围高度敏感，如果特征的值使用不同的度量标准，我们需要先对特征进行标准化处理  \n",
    "我们先通过以下几个步骤来概括以下算法的流程：\n",
    "1. 对原始d维数据集做标准化处理 5\\*2    \n",
    "2. 构造样本的协方差矩阵\n",
    "3. 计算协方差矩阵的特征值和相应的特征向量\n",
    "4. 选择与前k个最大特征值对应的特征向量，其中k为新特征空间的维度($k\\le d$)\n",
    "5. 通过前k个特征向量构建映射矩阵W\n",
    "6. 通过映射矩阵W将d维的输入数据集X转换到新的k维特征子空间 5  \n",
    "\n",
    "### 问题：如何理解主成分分析降维的原理？\n",
    "> 主成分简单来说就是对结果影响最大的成分，而成分包含在各个特征中  \n",
    "特征抽取的含义就是把成分从特征中抽取出来，将成分重新组合后映射到新的k维特征空间中  \n",
    "这种抽取并组合的原理是通过计算贡献方差来找出主成分，并通过映射矩阵将数据转换到k维特征空间  \n",
    "其中贡献方差是基于协方差矩阵转换成特征对中的特征值计算出来的，而映射矩阵是由特征对中的特征向量构造的 5  \n",
    "  \n",
    "\n",
    "## 5.1.1 总体方差与贡献方差\n",
    "> 本节介绍了如何通过协方差矩阵转换成特征对（特征值和特征向量），并使用特征值计算贡献方差的方法\n",
    "\n",
    "本节，我们将学习主成分分析算法的前四个步骤：数据标准化、构造协方差矩阵、获得协方差矩阵的特征值和特征向量，以及按降序排列特征值所对应的特征向量  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "'''\n",
    "python\tpandas\tdataframe\tto_csv() index\n",
    "\n",
    "df_winecsv = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',\n",
    "                      header=None)\n",
    "df_winecsv.columns = ['Class label', 'Alcohol', \n",
    "                   'Malic acid', 'Ash',\n",
    "                  'Alcalinity of ash', 'Magnesium',\n",
    "                  'Total phenols', 'Flavanoids',\n",
    "                  'Nonflavanoid phenols',\n",
    "                  'Proanthocyanins',\n",
    "                  'Color intensity', 'Hue',\n",
    "                  'OD280/OD315 of diluted wines',\n",
    "                  'Proline']\n",
    "df_winecsv.to_csv('../syc_data/py_machine_learning/wine.data', index=False)\n",
    "df_winecsv.head()\n",
    "'''\n",
    "\n",
    "df_wine = pd.read_csv('../syc_data/py_machine_learning/wine.data')\n",
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将葡萄酒数据集划分为训练集和测试集，并使用单位方差将其标准化  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tpandas\tdataframe\tvalues\n",
    "python\tsklearn\tModel Selection\ttrain_test_split()\n",
    "python\tsklearn\tPreprocessing and Normalization\tStandardScaler()\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成数据预处理后，我们进入第二步：构造协方差矩阵  \n",
    "$d\\times d$维协方差矩阵，其中d位数据集的维度，此矩阵成对地存储了不同特征之间地协方差，例如两个特征$x_j和x_k$ 可通过如下公式来计算协方差：   \n",
    "$$\\sigma_{jk}=\\frac{1}{n}\\sum_{i=1}^n{(x_j^{(i)}-\\mu_j)(x_k^{(i)}-\\mu_k)}$$\n",
    "在此，$\\mu_j$和$\\mu_k$分别位特征j和k的均值。注意，我们做了标准化处理后，样本的均值将为零   \n",
    "两个特征之间的协方差如果位正，说明它们会同时增减，而一个负的协方差值则表示两个特征会朝相反的方向变动，一个包含三个特征的协方差矩阵可记为：5  \n",
    "$$\\Sigma=\\left[\\begin{matrix}\\sigma^2_1&\\sigma_{12}&\\sigma_{13} \\cr \n",
    "    \\sigma_{21}&\\sigma^2_2&\\sigma_{23} \\cr \n",
    "    \\sigma_{31}&\\sigma_{32}&\\sigma^2_3 \\end{matrix}\\right]$$\n",
    "协方差矩阵的特征向量代表主成分（最大方差方向），而对应的特征值大小就决定了特征向量的重要性  \n",
    "### 问题：协方差矩阵其特征向量、特征值分别是什么？\n",
    "> 特征向量和特征值是协方差矩阵通过特征分解后得到，如葡萄酒中13个特征变量——13x13协方差矩阵——13个维特征向量及13个特征值，特征向量以列的方式存储于一个13x13维的矩阵中 5  \n",
    "\n",
    "就葡萄酒数据集来说，我们可以得到$13\\times13$维协方差矩阵的13个特征向量及其对应的特征值。特征值V需满足如下条件：  \n",
    "$$\\Sigma{V}=\\lambda{V}$$\n",
    "5  \n",
    "此处的特征值是$\\lambda$一个标量，我们来计算协方差矩阵的特征对  \n",
    "### 问题：什么是标量？\n",
    "> 几何代数的概念，对应于矢量，标量亦称作无向量，用通俗的说法，标量是只有大小，没有方向的量 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\tndarray.T\n",
    "python\tnumpy\tStatistics\tcov()\n",
    "python\tnumpy\tLinear algebra (numpy.linalg)\teig()\n",
    "'''\n",
    "import numpy as np\n",
    "cov_mat = np.cov(X_train_std.T) # 转换成协方差矩阵\n",
    "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat) # 将协方差矩阵进行特征分解得到特征值和特征向量\n",
    "print('如下一共{:d}个特征值'.format(len(eigen_vals))) # eigen_vals特征值 eigen_vecs 特征向量\n",
    "print('\\nEigenvalues|特征值 \\n{}'.format(eigen_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题：如何理解np.cov?\n",
    "> 公式 $$cov(X,Y)=\\frac{\\Sigma^n_{i=1}{(X_i-\\bar{X})(Y_i-\\bar{Y})}}{n-1}$$\n",
    "结果  $$C=\\left[\\begin{matrix}cov(1,1)&cov(1,2)&cov(1,3)&\\dots&cov(1,n) \\cr \n",
    "    cov(2,1)&cov(2,2)&cov(2,3)&\\dots&cov(2,n) \\cr \n",
    "    cov(3,1)&cov(3,2)&cov(3,3)&\\dots&cov(3,n) \\cr\n",
    "    \\dots&\\dots&\\dots&\\dots&\\dots \\cr\n",
    "    cov(n,1)&cov(n,2)&cov(n,3)&\\dots&cov(n,n)\n",
    "    \\end{matrix}\\right]$$\n",
    "\n",
    "协方差矩阵计算的是不同维度之间的协方差，而不是不同样本之间。拿到一个样本矩阵，首先要明确的就是行代表什么，列代表什么，如下所示 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tnumpy\tArray manipulation routines\tvstack()\n",
    "'''\n",
    "# 计算协方差的时候，一行代表一个特征\n",
    "# 下面计算cov(T, S, M)\n",
    "T = np.array([9, 15, 25, 14, 10, 18, 0, 16, 5, 19, 16, 20])\n",
    "S = np.array([39, 56, 93, 61, 50, 75, 32, 85, 42, 70, 66, 80])\n",
    "M = np.array([38, 56, 90, 63, 56, 77, 30, 80, 41, 79, 64, 88])\n",
    "X = np.vstack((T, S, M))\n",
    "# X每行代表一个属性\n",
    "#  每列代表一个示例，或者说观测\n",
    "print(np.cov(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为要将数据集压缩到一个新的特征子空间上来实现数据降维，所以我们只选择那些包含最多信息（方差最大）的特征向量(主成分)组成子集  \n",
    "由于特征值的大小决定了特征向量的重要性，因此需要将特征值按降序排列，我们感兴趣的是排序在前k个特征值所对应的特征向量。我们先绘制特征值的方差贡献率  \n",
    "特征值$\\lambda_j$的方差贡献率是指，特征值$\\lambda_j$与所有特征值和的比值 5  \n",
    "$$\\frac{\\lambda_j}{\\sum_{j=1}^d{\\lambda_j}}$$\n",
    "我们可以计算并绘制出方差贡献率的累积分布图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tBuilt-in Functions\tsum()\tsum()\n",
    "python\tBuilt-in Functions\tsorted()\tsorted() reverse\n",
    "python\tnumpy\tMathematical functions\tcumsum()\n",
    "python\tmatplotlib\tPyplot function overview\tbar() align\n",
    "python\tmatplotlib\tPyplot function overview\tstep()\n",
    "python\tmatplotlib\tPyplot function overview\tlegend() loc\n",
    "m\n",
    "'''\n",
    "tot = sum(eigen_vals) # 计算所有特征值之和\n",
    "var_exp = [(i/tot) for i in sorted(eigen_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(1,14), var_exp, alpha=0.5, align='center',\n",
    "       label='individual explained variance')\n",
    "plt.step(range(1,14), cum_var_exp, where='mid',\n",
    "        label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们应注意：PCA是一种无监督方法，意味着我们可以忽略类标信息，方差度量的是特征值在轴线上的分布 5  \n",
    "## 5.1.2 特征转换\n",
    "> 本节介绍了如何通过特征向量构建映射矩阵\n",
    "\n",
    "将协方差矩阵分解为特征对（特征值和特征向量）后，我们继续执行PCA方法的最后三个步骤，将葡萄酒数据集中的信息转换到新的主成分轴上  \n",
    "我们将对特征值按降序进行排列，并通过挑选出对应的特征向量构造出映射矩阵，然后使用映射矩阵将数据转换到低维的子空间上  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tMathematical functions\tabs()\n",
    "python\tSequence Types — list, tuple, range\tsort()\tsort() reverse\n",
    "'''\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n",
    "               for i in range(len(eigen_vals))]\n",
    "eigen_pairs.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出于演示的需要，我们只选择两个特征值最大的特征向量，这两个值之和占据了数据集总体方差的60% 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tArray manipulation routines\thstack()\n",
    "python\tnumpy\tConstants\tnewaxis\n",
    "'''\n",
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
    "               eigen_pairs[1][1][:, np.newaxis]))\n",
    "print('Matrix W:\\n{}'.format(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们得到了一个13x2维的映射矩阵W。通过映射矩阵，我们可以将样本转换为一个包含两个新特征的二维向量  \n",
    "$$x'=xW$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\tdot()\n",
    "'''\n",
    "X_train_pca = X_train_std.dot(w)\n",
    "X_train_std[0].dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转化后的葡萄酒数据集将以124x2维矩阵的方式存储，我们以散点图的方式来对其进行可视化展示 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tArray manipulation routines\tunique()\n",
    "python\tnumpy\tArray manipulation routines\tscatter() marker|c\n",
    "'''\n",
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "    plt.scatter(X_train_pca[y_train==l, 0],\n",
    "                X_train_pca[y_train==l, 1],\n",
    "                c=c, label=l, marker=m\n",
    "                )\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相较于第二主成分（y轴），数据更多地沿着x轴（第一主成分）方向分布，线性分类器能够很好地对其进行划分 5   \n",
    "## 5.1.3 使用scikit-learn进行主成分分析\n",
    "> 介绍了如何使用sklearn中的PCA算法\n",
    "\n",
    "我们使用sklearn中的PCA对葡萄酒数据集做预处理，然后使用逻辑斯蒂回归对转换后的数据进行分类  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_decision_regions(X, y, classifier,\n",
    "                          test_idx = None, resolution = 0.02):\n",
    "    # 设置散点生成器和色图\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    \n",
    "    # 绘制决策边界\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                          np.arange(x2_min, x2_max, resolution)) \n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha = 0.4, cmap = cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    \n",
    "    # 绘制样本分类\n",
    "    X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x = X[y == cl, 0], y = X[y == cl, 1],\n",
    "                   alpha = 0.8, color = cmap(idx),\n",
    "                   marker = markers[idx], label = cl)\n",
    "\n",
    "    # 高亮测试样本\n",
    "    if test_idx:\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1], c = 'yellow',\n",
    "                   alpha = 0.5, linewidth = 1, marker = 'o',\n",
    "                   s = 55, label = 'test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tMatrix Decomposition\tPCA()\n",
    "python\tsklearn\tLinear Models\tLogisticRegression()\n",
    "'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "lr = LogisticRegression()\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "lr.fit(X_train_pca, y_train)\n",
    "plot_decision_regions(X_train_pca, y_train, classifier=lr)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与我们自己实现的PCA相比较的话，点的分布在y轴方向上是相反的，出现这个现象的原因是特征向量可以为正或负 5  \n",
    "接着我们看一下应用在转换后测试数据上的效果  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X_test_pca, y_test, classifier=lr)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通过将PCA类中n_componets参数设为None，从而可以保留所有映射后的特征，方差贡献率则可以用explained_variance_ratio_属性进行查询  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tsklearn\tMatrix Decomposition\tPCA()\n",
    "python\tsklearn\tMatrix Decomposition\tPCA() explained_variance_ratio_\n",
    "'''\n",
    "pca = PCA(n_components=None)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 通过线性判别分析压缩无监督数据\n",
    "> 介绍了线性判别分析(LDA)的原理、实现和使用方法\n",
    "\n",
    "线性判别分析（LDA）也是一种特征抽取技术  \n",
    "LDA和PCA都是将原始特征数据集映射到新的特征子空间，而且都是用于降低数据维度的线性转换技巧  \n",
    "LDA的目标是使得映射后的特征集（往往小于原有特征空间）能够获得最佳分类效果  \n",
    "LDA是监督算法，PCA是无监督算法，所以在面向分类问题上LDA要优于PCA，但A.M.Martinez提出在图像识别的某些情况下，如果每个类别只有少量样本，使用PCA更佳 5  \n",
    "\n",
    "### 问题：什么是线性转换？\n",
    "> 也叫线性变换、线性映射，是线性代数研究的一个对象，是通过加法或乘法从一个向量空间变换到另一个向量空间的过程 5  \n",
    "\n",
    "LDA是Ronald A.Fisher于1936年针对二类别分类问题对Fisher线性判别做了最初的形式化。1948年Radhakrishna Rao通过不同类别方差相等和类别内样本呈标准正态分布的假设，将LDA泛化到了多类别分类问题上    \n",
    "下图用于解释二类别分类中LDA的概念  \n",
    "![5-2](../syn_pic/py_machine_learning/5-2.png)\n",
    "在X轴（LD1）方向上，通过钟型曲线的波峰可以很好的将两个类分开，LD2则无法提供关于类别区分的任何信息  \n",
    "**一个关于LDA的假设就是数据呈正态分布**,另外还假定各类别中数据具有相同的协方差矩阵，样本的特征从统计上来讲是相互独立的 5  \n",
    "从实际应用中，即使一个或多个假设没有满足，LDA仍旧可以很好地完成降维工作  \n",
    "LDA方法的关键步骤如下：\n",
    "1. 对d维数据集进行标准化处理(d为特征数量)\n",
    "2. 对于每一类别，计算d维的均值向量  \n",
    "3. 构建类间的散布矩阵$S_B$以及类内的散布矩阵$S_W$ 5\n",
    "4. 计算矩阵$S_W^{-1}S_B$的特征值及对应的特征向量  \n",
    "5. 选取前k个特征值所对应的特征向量，构造一个$d\\times k$维的转换矩阵W,其中特征向量以列的形式排列\n",
    "6. 使用转换矩阵W将样本映射到新的特征子空间上 5  \n",
    "\n",
    "### 问题：什么是均值向量？\n",
    "> 随机向量的数学期望，就是各个特征的均值构成的向量 5  \n",
    "\n",
    "## 5.2.1 计算散布矩阵\n",
    "> 本节介绍了计算类内和类间散布矩阵的公式和方法\n",
    "\n",
    "### 问题：什么是散布矩阵？\n",
    "> 也叫散度矩阵、类内离散度矩阵或者类内离差阵，散度矩阵=协方差矩阵*(n-1)，其中n表示样本的个数 5  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
