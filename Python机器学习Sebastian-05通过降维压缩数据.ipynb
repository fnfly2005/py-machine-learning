{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本章简介\n",
    "另一种常用于降维方法就是特征抽取，特征抽取可以将原始数据集变化到一个维度更低的新的特征子空间，从而对数据进行压缩，避免维度灾难  \n",
    "本章读者将学习三种降维技术：  \n",
    "+ 无监督数据压缩——主成分分析(PCA)\n",
    "+ 基于类别可分最大化的监督降维技术——线性判别分析(LDA)\n",
    "+ 通过核主成分分析进行非线性降维 5  \n",
    "\n",
    "# 5.1 无监督数据降维技术-主成分分析\n",
    "> 本章介绍了主成分分析(PCA)的原理、实现和使用方法\n",
    "\n",
    "### 问题：特征选择和特征抽取有什么区别？\n",
    "> 特征抽取后的新特征是原来特征的一个映射，而特征选择后的特征是原来特征的一个子集 5  \n",
    "\n",
    "特征抽取算法会将数据转换或者映射到一个新的特征空间，可以理解为：在尽可能保持相关信息的情况下，对数据进行压缩的一种方法  \n",
    "特征抽取通常用于提高计算效率，同样也可以帮助我们降低\"维度灾难\"——尤其当模型不适于正则化处理时  \n",
    "主成分分析(PCA)是一种广泛应用于不同领域的无监督线性数据转换技术，其突出作用是降维  \n",
    "简而言之，PCA的目标是在高维数据中找到最大方差的方向，并将数据映射到一个维度不大于原始数据的新的子空间上 5  \n",
    "如下图所示，新的子空间上正交的坐标轴（主成分）可被解释为方差最大的方向。在此，$x_1$和$x_2$为原始特征的坐标轴，而PC1和PC2即为主成分  \n",
    "![5-1](../syn_pic/py_machine_learning/5-1.png)\n",
    "如果使用PCA降维，我们将构建一个$d\\times k$维的转换矩阵W，这样就可以将一个样本向量x映射到一个新的k维特征子空间上去，此空间的维度小于原始的d维特征空间：  \n",
    "$$x=[x_1,x_2,\\dots,x_d]，x\\in{R}^d$$\n",
    "$$\\downarrow xW,W\\in{R}^{d\\times k}$$\n",
    "$$z=[z_1,z_2,\\dots,z_k],z\\in{R}^k$$\n",
    "完成从原始d维数据到新的k维子空间(一般情况下$k\\ll d$)的转换后，第一主成分的方差应该是最大的，由于各主成分之间是不相关的（正交的），后续各主成分也具备尽可能大的方差  \n",
    "需要注意的是，主成分的方向对数据值的范围高度敏感，如果特征的值使用不同的度量标准，我们需要先对特征进行标准化处理  \n",
    "我们先通过以下几个步骤来概括以下算法的流程：\n",
    "1. 对原始d维数据集做标准化处理 5\\*2    \n",
    "2. 构造样本的协方差矩阵\n",
    "3. 计算协方差矩阵的特征值和相应的特征向量\n",
    "4. 选择与前k个最大特征值对应的特征向量，其中k为新特征空间的维度($k\\le d$)\n",
    "5. 通过前k个特征向量构建映射矩阵W\n",
    "6. 通过映射矩阵W将d维的输入数据集X转换到新的k维特征子空间 5  \n",
    "\n",
    "### 问题：如何理解主成分分析降维的原理？\n",
    "> 主成分简单来说就是对结果影响最大的成分，而成分包含在各个特征中  \n",
    "特征抽取的含义就是把成分从特征中抽取出来，将成分重新组合后映射到新的k维特征空间中  \n",
    "这种抽取并组合的原理是通过计算贡献方差来找出主成分，并通过映射矩阵将数据转换到k维特征空间  \n",
    "其中贡献方差是基于协方差矩阵转换成特征对中的特征值计算出来的，而映射矩阵是由特征对中的特征向量构造的 5  \n",
    "  \n",
    "\n",
    "## 5.1.1 总体方差与贡献方差\n",
    "> 本节介绍了如何通过协方差矩阵转换成特征对（特征值和特征向量），并使用特征值计算贡献方差的方法\n",
    "\n",
    "本节，我们将学习主成分分析算法的前四个步骤：数据标准化、构造协方差矩阵、获得协方差矩阵的特征值和特征向量，以及按降序排列特征值所对应的特征向量  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "'''\n",
    "python\tpandas\tdataframe\tto_csv() index\n",
    "\n",
    "df_winecsv = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',\n",
    "                      header=None)\n",
    "df_winecsv.columns = ['Class label', 'Alcohol', \n",
    "                   'Malic acid', 'Ash',\n",
    "                  'Alcalinity of ash', 'Magnesium',\n",
    "                  'Total phenols', 'Flavanoids',\n",
    "                  'Nonflavanoid phenols',\n",
    "                  'Proanthocyanins',\n",
    "                  'Color intensity', 'Hue',\n",
    "                  'OD280/OD315 of diluted wines',\n",
    "                  'Proline']\n",
    "df_winecsv.to_csv('../syc_data/py_machine_learning/wine.data', index=False)\n",
    "df_winecsv.head()\n",
    "'''\n",
    "\n",
    "df_wine = pd.read_csv('../syc_data/py_machine_learning/wine.data')\n",
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将葡萄酒数据集划分为训练集和测试集，并使用单位方差将其标准化  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tpandas\tdataframe\tvalues\n",
    "python\tsklearn\tModel Selection\ttrain_test_split()\n",
    "python\tsklearn\tPreprocessing and Normalization\tStandardScaler()\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成数据预处理后，我们进入第二步：构造协方差矩阵  \n",
    "$d\\times d$维协方差矩阵，其中d位数据集的维度，此矩阵成对地存储了不同特征之间地协方差，例如两个特征$x_j和x_k$ 可通过如下公式来计算协方差：   \n",
    "$$\\sigma_{jk}=\\frac{1}{n}\\sum_{i=1}^n{(x_j^{(i)}-\\mu_j)(x_k^{(i)}-\\mu_k)}$$\n",
    "在此，$\\mu_j$和$\\mu_k$分别位特征j和k的均值。注意，我们做了标准化处理后，样本的均值将为零   \n",
    "两个特征之间的协方差如果位正，说明它们会同时增减，而一个负的协方差值则表示两个特征会朝相反的方向变动，一个包含三个特征的协方差矩阵可记为：5  \n",
    "$$\\Sigma=\\left[\\begin{matrix}\\sigma^2_1&\\sigma_{12}&\\sigma_{13} \\cr \n",
    "    \\sigma_{21}&\\sigma^2_2&\\sigma_{23} \\cr \n",
    "    \\sigma_{31}&\\sigma_{32}&\\sigma^2_3 \\end{matrix}\\right]$$\n",
    "协方差矩阵的特征向量代表主成分（最大方差方向），而对应的特征值大小就决定了特征向量的重要性  \n",
    "### 问题：协方差矩阵其特征向量、特征值分别是什么？\n",
    "> 特征向量和特征值是协方差矩阵通过特征分解后得到，如葡萄酒中13个特征变量——13x13协方差矩阵——13个维特征向量及13个特征值，特征向量以列的方式存储于一个13x13维的矩阵中 5  \n",
    "\n",
    "就葡萄酒数据集来说，我们可以得到$13\\times13$维协方差矩阵的13个特征向量及其对应的特征值。特征值V需满足如下条件：  \n",
    "$$\\Sigma{V}=\\lambda{V}$$\n",
    "5  \n",
    "此处的特征值是$\\lambda$一个标量，我们来计算协方差矩阵的特征对  \n",
    "### 问题：什么是标量？\n",
    "> 几何代数的概念，对应于矢量，标量亦称作无向量，用通俗的说法，标量是只有大小，没有方向的量 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\tndarray.T\n",
    "python\tnumpy\tStatistics\tcov()\n",
    "python\tnumpy\tLinear algebra (numpy.linalg)\teig()\n",
    "'''\n",
    "import numpy as np\n",
    "cov_mat = np.cov(X_train_std.T) # 转换成协方差矩阵\n",
    "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat) # 将协方差矩阵进行特征分解得到特征值和特征向量\n",
    "print('如下一共{:d}个特征值'.format(len(eigen_vals))) # eigen_vals特征值 eigen_vecs 特征向量\n",
    "print('\\nEigenvalues|特征值 \\n{}'.format(eigen_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题：如何理解np.cov?\n",
    "> 公式 $$cov(X,Y)=\\frac{\\Sigma^n_{i=1}{(X_i-\\bar{X})(Y_i-\\bar{Y})}}{n-1}$$\n",
    "结果  $$C=\\left[\\begin{matrix}cov(1,1)&cov(1,2)&cov(1,3)&\\dots&cov(1,n) \\cr \n",
    "    cov(2,1)&cov(2,2)&cov(2,3)&\\dots&cov(2,n) \\cr \n",
    "    cov(3,1)&cov(3,2)&cov(3,3)&\\dots&cov(3,n) \\cr\n",
    "    \\dots&\\dots&\\dots&\\dots&\\dots \\cr\n",
    "    cov(n,1)&cov(n,2)&cov(n,3)&\\dots&cov(n,n)\n",
    "    \\end{matrix}\\right]$$\n",
    "\n",
    "协方差矩阵计算的是不同维度之间的协方差，而不是不同样本之间。拿到一个样本矩阵，首先要明确的就是行代表什么，列代表什么，如下所示 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tnumpy\tArray manipulation routines\tvstack()\n",
    "'''\n",
    "# 计算协方差的时候，一行代表一个特征\n",
    "# 下面计算cov(T, S, M)\n",
    "T = np.array([9, 15, 25, 14, 10, 18, 0, 16, 5, 19, 16, 20])\n",
    "S = np.array([39, 56, 93, 61, 50, 75, 32, 85, 42, 70, 66, 80])\n",
    "M = np.array([38, 56, 90, 63, 56, 77, 30, 80, 41, 79, 64, 88])\n",
    "Xs = np.vstack((T, S, M))\n",
    "# X每行代表一个属性\n",
    "#  每列代表一个示例，或者说观测\n",
    "print(np.cov(Xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为要将数据集压缩到一个新的特征子空间上来实现数据降维，所以我们只选择那些包含最多信息（方差最大）的特征向量(主成分)组成子集  \n",
    "由于特征值的大小决定了特征向量的重要性，因此需要将特征值按降序排列，我们感兴趣的是排序在前k个特征值所对应的特征向量。我们先绘制特征值的方差贡献率  \n",
    "特征值$\\lambda_j$的方差贡献率是指，特征值$\\lambda_j$与所有特征值和的比值 5  \n",
    "$$\\frac{\\lambda_j}{\\sum_{j=1}^d{\\lambda_j}}$$\n",
    "我们可以计算并绘制出方差贡献率的累积分布图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tBuilt-in Functions\tsum()\tsum()\n",
    "python\tBuilt-in Functions\tsorted()\tsorted() reverse\n",
    "python\tnumpy\tMathematical functions\tcumsum()\n",
    "python\tmatplotlib\tPyplot function overview\tbar() align\n",
    "python\tmatplotlib\tPyplot function overview\tstep()\n",
    "python\tmatplotlib\tPyplot function overview\tlegend() loc\n",
    "m\n",
    "'''\n",
    "tot = sum(eigen_vals) # 计算所有特征值之和\n",
    "var_exp = [(i/tot) for i in sorted(eigen_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(range(1,14), var_exp, alpha=0.5, align='center',\n",
    "       label='individual explained variance')\n",
    "plt.step(range(1,14), cum_var_exp, where='mid',\n",
    "        label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们应注意：PCA是一种无监督方法，意味着我们可以忽略类标信息，方差度量的是特征值在轴线上的分布 5  \n",
    "## 5.1.2 特征转换\n",
    "> 本节介绍了如何通过特征向量构建映射矩阵\n",
    "\n",
    "将协方差矩阵分解为特征对（特征值和特征向量）后，我们继续执行PCA方法的最后三个步骤，将葡萄酒数据集中的信息转换到新的主成分轴上  \n",
    "我们将对特征值按降序进行排列，并通过挑选出对应的特征向量构造出映射矩阵，然后使用映射矩阵将数据转换到低维的子空间上  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tMathematical functions\tabs()\n",
    "python\tSequence Types — list, tuple, range\tsort()\tsort() reverse\n",
    "'''\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])\n",
    "               for i in range(len(eigen_vals))]\n",
    "eigen_pairs.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出于演示的需要，我们只选择两个特征值最大的特征向量，这两个值之和占据了数据集总体方差的60% 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tArray manipulation routines\thstack()\n",
    "python\tnumpy\tConstants\tnewaxis\n",
    "'''\n",
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
    "               eigen_pairs[1][1][:, np.newaxis]))\n",
    "print('Matrix W:\\n{}'.format(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们得到了一个13x2维的映射矩阵W。通过映射矩阵，我们可以将样本转换为一个包含两个新特征的二维向量  \n",
    "$$x'=xW$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\tdot()\n",
    "'''\n",
    "X_train_pca = X_train_std.dot(w)\n",
    "X_train_std[0].dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转化后的葡萄酒数据集将以124x2维矩阵的方式存储，我们以散点图的方式来对其进行可视化展示 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tArray manipulation routines\tunique()\n",
    "python\tnumpy\tArray manipulation routines\tscatter() marker|c\n",
    "'''\n",
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "    plt.scatter(X_train_pca[y_train==l, 0],\n",
    "                X_train_pca[y_train==l, 1],\n",
    "                c=c, label=l, marker=m\n",
    "                )\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相较于第二主成分（y轴），数据更多地沿着x轴（第一主成分）方向分布，线性分类器能够很好地对其进行划分 5   \n",
    "## 5.1.3 使用scikit-learn进行主成分分析\n",
    "> 介绍了如何使用sklearn中的PCA算法\n",
    "\n",
    "我们使用sklearn中的PCA对葡萄酒数据集做预处理，然后使用逻辑斯蒂回归对转换后的数据进行分类  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_decision_regions(X, y, classifier,\n",
    "                          test_idx = None, resolution = 0.02):\n",
    "    # 设置散点生成器和色图\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    \n",
    "    # 绘制决策边界\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                          np.arange(x2_min, x2_max, resolution)) \n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha = 0.4, cmap = cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    \n",
    "    # 绘制样本分类\n",
    "    X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x = X[y == cl, 0], y = X[y == cl, 1],\n",
    "                   alpha = 0.8, color = cmap(idx),\n",
    "                   marker = markers[idx], label = cl)\n",
    "\n",
    "    # 高亮测试样本\n",
    "    if test_idx:\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1], c = 'yellow',\n",
    "                   alpha = 0.5, linewidth = 1, marker = 'o',\n",
    "                   s = 55, label = 'test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tMatrix Decomposition\tPCA()\n",
    "python\tsklearn\tLinear Models\tLogisticRegression()\n",
    "'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "lr = LogisticRegression()\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "lr.fit(X_train_pca, y_train)\n",
    "plot_decision_regions(X_train_pca, y_train, classifier=lr)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与我们自己实现的PCA相比较的话，点的分布在y轴方向上是相反的，出现这个现象的原因是特征向量可以为正或负 5  \n",
    "接着我们看一下应用在转换后测试数据上的效果  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X_test_pca, y_test, classifier=lr)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通过将PCA类中n_componets参数设为None，从而可以保留所有映射后的特征，方差贡献率则可以用explained_variance_ratio_属性进行查询  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tsklearn\tMatrix Decomposition\tPCA()\n",
    "python\tsklearn\tMatrix Decomposition\tPCA() explained_variance_ratio_\n",
    "'''\n",
    "pca = PCA(n_components=None)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 通过线性判别分析压缩无监督数据\n",
    "> 介绍了线性判别分析(LDA)的原理、实现和使用方法\n",
    "\n",
    "线性判别分析（LDA）也是一种特征抽取技术  \n",
    "LDA和PCA都是将原始特征数据集映射到新的特征子空间，而且都是用于降低数据维度的线性转换技巧  \n",
    "LDA的目标是使得映射后的特征集（往往小于原有特征空间）能够获得最佳分类效果  \n",
    "LDA是监督算法，PCA是无监督算法，所以在面向分类问题上LDA要优于PCA，但A.M.Martinez提出在图像识别的某些情况下，如果每个类别只有少量样本，使用PCA更佳 5  \n",
    "\n",
    "### 问题：什么是线性转换？\n",
    "> 也叫线性变换、线性映射，是线性代数研究的一个对象，是通过加法或乘法从一个向量空间变换到另一个向量空间的过程 5  \n",
    "\n",
    "LDA是Ronald A.Fisher于1936年针对二类别分类问题对Fisher线性判别做了最初的形式化。1948年Radhakrishna Rao通过不同类别方差相等和类别内样本呈标准正态分布的假设，将LDA泛化到了多类别分类问题上    \n",
    "下图用于解释二类别分类中LDA的概念  \n",
    "![5-2](../syn_pic/py_machine_learning/5-2.png)\n",
    "在X轴（LD1）方向上，通过钟型曲线的波峰可以很好的将两个类分开，LD2则无法提供关于类别区分的任何信息  \n",
    "**一个关于LDA的假设就是数据呈正态分布**,另外还假定各类别中数据具有相同的协方差矩阵，样本的特征从统计上来讲是相互独立的 5  \n",
    "从实际应用中，即使一个或多个假设没有满足，LDA仍旧可以很好地完成降维工作  \n",
    "LDA方法的关键步骤如下：\n",
    "1. 对d维数据集进行标准化处理(d为特征数量)\n",
    "2. 对于每一类别，计算d维的均值向量  \n",
    "3. 构建类间的散布矩阵$S_B$以及类内的散布矩阵$S_W$ 5\n",
    "4. 计算矩阵$S_W^{-1}S_B$的特征值及对应的特征向量  \n",
    "5. 选取前k个特征值所对应的特征向量，构造一个$d\\times k$维的转换矩阵W,其中特征向量以列的形式排列\n",
    "6. 使用转换矩阵W将样本映射到新的特征子空间上 5  \n",
    "\n",
    "### 问题：什么是均值向量？\n",
    "> 随机向量的数学期望，就是各个特征的均值构成的向量 5  \n",
    "\n",
    "## 5.2.1 计算散布矩阵\n",
    "> 本节介绍了计算类内和类间散布矩阵的公式和方法\n",
    "\n",
    "### 问题：什么是散布矩阵？\n",
    "> 也叫散度矩阵、类内离散度矩阵或者类内离差阵，散度矩阵=协方差矩阵*(n-1)，其中n表示样本的个数，协方差矩阵可以看作是归一化的散布矩阵 5  \n",
    "\n",
    "由于葡萄酒数据集已经做过标准化处理，我们就直接计算均值向量，均值向量$m_i$存储了类别i中样本的特征均值$\\mu_m$  \n",
    "$$m_i=\\frac{1}{n_i}\\sum^c_{x\\in{D_i}}{x_m}$$\n",
    "葡萄酒数据集的三个类别对应三个均值向量：  \n",
    "$$m_i=\\left[\\begin{matrix}\\mu_{i,alcohol} \\cr \\mu_{i,malic acid} \\cr \\dots \\cr \\mu_{i,proline} \\end{matrix}\\right] i\\in\\{1,2,3\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tnumpy\tInput and output\tset_printoptions()\n",
    "python\tnumpy\tStatistics\tmean()\n",
    "'''\n",
    "np.set_printoptions(precision=4)\n",
    "mean_vecs = []\n",
    "for label in range(1,4):\n",
    "    mean_vecs.append(np.mean(\n",
    "                    X_train_std[y_train==label], axis=0))\n",
    "    print('MV {}: {}\\n'.format(label, mean_vecs[label-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过均值向量，我们来计算类内散布矩阵$S_W$:  \n",
    "$$S_W=\\sum^c_{i=1}{S_i}$$\n",
    "这可以通过累加各类别i的散布矩阵$S_i$来计算：  \n",
    "$$S_i=\\sum^c_{x\\in{D_i}}{(x-m_i)(x-m_i)^T}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\tshape\n",
    "\n",
    "'''\n",
    "d = 13 # number of features\n",
    "S_W = np.zeros((d, d))\n",
    "for label,mv in zip(range(1,4), mean_vecs):\n",
    "    class_scatter = np.zeros((d, d))\n",
    "    for row in X[y == label]:\n",
    "        row, mv = row.reshape(d, 1), mv.reshape(d, 1)\n",
    "        class_scatter += (row-mv).dot((row-mv).T)\n",
    "    S_W += class_scatter\n",
    "print('Within-class scatter matrix: {}x{}'.format(\n",
    "                    S_W.shape[0], S_W.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们假设训练集的类标是均匀分布的，但是，实际情况并非如此  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tStatistics\tbincount()\n",
    "'''\n",
    "print('Class label distribution:{}'.format(np.bincount(y_train)[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，在我们计算散布矩阵$S_W$前，需要对各类别的散布矩阵$S_i$做缩放处理  \n",
    "计算散布矩阵的方式与计算协方差矩阵$\\sum_i$的方式是一致的。我们用各类别单独的散布矩阵除以此类别内样本数量$N_i$  \n",
    "$$\\sum_i=\\frac{1}{N_i}S_w=\\frac{1}{N_i}\\sum^c_{x\\in{D_i}}{(x-m_i)(x-m_i)^T}$$\n",
    "5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tStatistics\tcov()\n",
    "'''\n",
    "d = 13 # number of features\n",
    "S_W = np.zeros((d, d))\n",
    "for label,mv in zip(range(1,4), mean_vecs):\n",
    "    class_scatter = np.cov(X_train_std[y_train==label].T)\n",
    "    S_W += class_scatter\n",
    "print('Scaled Within-class scatter matrix: {}x{}'.format(\n",
    "                    S_W.shape[0], S_W.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上是类内散布矩阵的计算过程，随后我们来计算类间散布矩阵$S_B$  \n",
    "$$S_B=\\sum^c_{i=1}{N_i(m_i-m)(m_i-m)^T}$$\n",
    "其中，m为全局均值，它在计算时用到了所有类别中的全部样本  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tBuilt-in Functions\tenumerate()\tenumerate()\n",
    "'''\n",
    "mean_overall = np.mean(X_train_std, axis=0)\n",
    "d = 13 # number of features\n",
    "S_B = np.zeros((d, d))\n",
    "for i, mean_vec in enumerate(mean_vecs):\n",
    "    n = X[y==i+1, :].shape[0]\n",
    "    mean_vec = mean_vec.reshape(d, 1)\n",
    "    mean_overall = mean_overall.reshape(d, 1)\n",
    "S_B += n * (mean_vec - mean_overall).dot((mean_vec - mean_overall).T)\n",
    "print('Between-class scatter matrix: {}x{}'.format(\n",
    "                    S_B.shape[0], S_B.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.2 在新特征子空间上选取线性判别算法\n",
    "> 介绍了如何通过类内和类间散布矩阵计算特征对，并利用特征值来度量类别区分能力\n",
    "\n",
    "LDA接下来的步骤类似PCA，不过PCA是特征分解，LDA是求解$S_W^{-1}S_B$的特征对（特征向量和特征值）： \n",
    "### 问题：什么是逆矩阵？\n",
    "> $S_W^{-1}$属于逆矩阵，设A是一个n阶矩阵，若存在另一个n阶矩阵B，使得： AB=BA=E ，则称方阵A可逆，并称方阵B是A的逆矩阵 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tLinear algebra (numpy.linalg)\tlinalg.inv()\n",
    "python\tnumpy\tLinear algebra (numpy.linalg)\tlinalg.eig()\n",
    "'''\n",
    "eigen_vals, eigen_vecs = \\\n",
    "np.linalg.eig(np.linalg.inv(S_W).dot(S_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随后我们按照降序对特征值进行排序：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tBuilt-in Functions\tsorted()\tsorted() reverse\n",
    "python\tBuilt-in Functions\tsorted()\tsorted() key\n",
    "'''\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i])\n",
    "               for i in range(len(eigen_vals))]\n",
    "eigen_pairs = sorted(eigen_pairs,\n",
    "                    key=lambda k: k[0], reverse=True)\n",
    "print('Eigenvalues in decreasing order:\\n')\n",
    "for eigen_val in eigen_pairs:\n",
    "    print(eigen_val[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$d\\times d$维协方差矩阵的秩最大为d-1，我们只得到了一个非零特征值，实际得到的第2-13个特征值并非完全为零，是几乎等于0的实数，这是因为Numpy的精度所致  \n",
    "我们达到了极少情况下的完美共线性，因为矩阵只有一个含非零特征值的特征向量，这时协方差矩阵的秩为1  \n",
    "为了度量线性判别可以获取多少可区分类别的信息，我们按照特征值降序绘制出线性判别信息保持程度的图像  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\treal\n",
    "python\tnumpy\tMathematical functions\tcumsum()\n",
    "python\tmatplotlib\tPyplot function overview\tstep()\n",
    "'''\n",
    "tot = sum(eigen_vals.real)\n",
    "discr = [(i / tot) for i in sorted(eigen_vals.real, reverse=True)]\n",
    "cum_discr = np.cumsum(discr)\n",
    "plt.bar(range(1, 14), discr, alpha=0.5, align='center',\n",
    "       label='individual \"discriminability\"')\n",
    "plt.step(range(1, 14), cum_discr, where='mid',\n",
    "        label='cumulative \"discriminability\"')\n",
    "plt.ylabel('\"discriminability\" ratio')\n",
    "plt.xlabel('Linear Discriminants')\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到前1个线性判别几乎获取了全部有用信息 5  \n",
    "下面我们来构建转换矩阵W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tnumpy\tConstants\tnewaxis\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\treal\n",
    "python\tnumpy\tArray manipulation routines\thstack()\n",
    "'''\n",
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis].real,\n",
    "              eigen_pairs[1][1][:, np.newaxis].real))\n",
    "print('Matrix W:\\n', w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.3 将样本映射到新的特征空间\n",
    "> 介绍了如何通过转换矩阵W将样本映射到新的特征空间及映射后的效果  \n",
    "\n",
    "我们可以通过乘积的方式对训练数据集进行转换：  \n",
    "$$X'=XW$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tArray manipulation routines\tscatter() marker|c\n",
    "'''\n",
    "X_train_lda = X_train_std.dot(w)\n",
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "    plt.scatter(X_train_lda[y_train==l, 0],\n",
    "               X_train_lda[y_train==l, 1],\n",
    "               c=c, label=l, marker=m)\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过结果图像可见，三个葡萄酒类在特征LD1是线性可分的： 5  \n",
    "## 5.2.4 使用scikit-learn进行LDA分析\n",
    "> 本节介绍了如何基于sklearn使用LDA来进行降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tDiscriminant Analysis\tLinearDiscriminantAnalysis\n",
    "'''\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "x_train_lda = lda.fit_transform(X_train_std, y_train)\n",
    "x_train_lda[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用逻辑回归来观察在转换后数据上的表现  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr = lr.fit(X_train_lda, y_train)\n",
    "plot_decision_regions(X_train_lda, y_train, classifier=lr)\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面，我们再看一下模型在测试数据集上的效果 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "'''\n",
    "X_test_lda = lda.transform(X_test_std)\n",
    "plot_decision_regions(X_test_lda, y_test, classifier=lr)\n",
    "plt.xlabel('LD 1')\n",
    "plt.ylabel('LD 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 使用核主成分分析进行非线性映射\n",
    "> 本章介绍了通过核技巧PCA解决非线性问题降维的原理、实现方法、基于sklearn的应用  \n",
    "\n",
    "在实际应用中，对于大多数情况下，我们面对非线性问题使用线性降维不是最好的方法，本章将介绍使用核PCA将数据映射到线性可分的低维空间上 5  \n",
    "## 5.3.1 核函数与核技巧\n",
    "> 介绍了使用基于相似(核)矩阵的技巧来实现映射的原理\n",
    "\n",
    "第3章我们讲解SVM原理时，曾谈过用核技巧将线性不可分的特征映射到更高维线性可分的特征空间里  \n",
    "定义如下的映射函数$\\phi$:  \n",
    "$$\\phi:R^d\\to R^k(k\\gg d)$$\n",
    "映射函数$\\phi$可以将原有特征组合成新的特征，将原始的d维数据转换成更高维的k维特征空间 5  \n",
    "举例来说：对于二维特征向量$x\\in R^d$（这个表达式意思x是包含d个特征的列向量），采用如下映射过程转换到三维空间  \n",
    "$$x=[x_1,x_2]^T$$\n",
    "$$\\downarrow\\phi$$\n",
    "$$z=[x_1^2,\\sqrt{2x_1x_2},x_2^2]^T$$\n",
    "### 问题：什么是列向量？\n",
    "> 在线性代数中，列向量是一个 n×1 的矩阵，即矩阵由一个含有n个元素的列所组成 5  \n",
    "\n",
    "所以核PCA的基本理念，是通过非线性映射函数将数据转换到高维空间，然后在高维空间中使用标准PCA将其映射到低维空间  \n",
    "但这种方法会带来一个问题，就是这样计算时的成本非常高，所以我们需要使用核技巧来降低计算成本，核技巧的理念是在原始特征空间中计算两个高维特征空间中向量的相似度 5  \n",
    "标准PCA方法中特征k和特征j之间协方差的公式如下  \n",
    "$$\\sigma_{jk}=\\frac{1}{n}\\sum_{i=1}^n{(x_j^{(i)}-\\mu_j)(x_k^{(i)}-\\mu_k)}$$\n",
    "由于标准化处理后的均值为0，上述公式简化为：  \n",
    "$$\\sigma_{jk}=\\frac{1}{n}\\sum_{i=1}^n{x_j^{(i)}x_k^{(i)}}$$\n",
    "下面是计算协方差矩阵$\\sigma$的公式 5  \n",
    "Bernhard Schoellkopf提出了一种方法，可以使用$\\phi$通过在原始特征空间上的非线性特征组合来替代样本间点积的计算：  \n",
    "$$\\Sigma=\\frac{1}{n}\\sum_{i=1}^n{\\phi(x^{(i)})\\phi(x^{(i)})^T}$$\n",
    "为了求得特征向量，我们求解下述公式：  \n",
    "$$\\Sigma{v}=\\lambda{v}$$\n",
    "$$\\Rightarrow\\frac{1}{n}\\sum_{i=1}^n{\\phi(x^{(i)})\\phi(x^{(i)})^T}v=\\lambda{v}$$\n",
    "$$\\Rightarrow v=\\frac{1}{n\\lambda}\\sum_{i=1}^n{\\phi(x^{(i)})[\\phi(x^{(i)})^T}v]$$\n",
    "5  \n",
    "其中, $\\lambda$和$v$分别为协方差矩阵$\\Sigma$的特征值和特征向量  \n",
    "注意到等式右边方括号内为标量，上式表明，当$\\lambda\\ne0$时，对应的特征向量v可以表示为所有$\\phi(x_i)$的线性组合[1]，即  \n",
    "$$v=\\frac{1}{n}\\sum_{i=1}^n{a^{(i)}\\phi(x^{(i)})}$$\n",
    "5  \n",
    "这里的a可以通过提取核（相似）矩阵K的特征向量来得到  \n",
    "核矩阵的推导过程如下：  \n",
    "首先，使用矩阵符号来表示协方差矩阵，其中$\\phi(X)$是一个$n\\times k$维的矩阵：  \n",
    "$$\\Sigma=\\frac{1}{n}\\sum_{i=1}^n{\\phi(x^{(i)})\\phi(x^{(i)})^T}=\\frac{1}{n}\\phi(X)[\\phi(X)]^T$$\n",
    "我们可以将特征向量的公式记为： 5  \n",
    "$$v=\\frac{1}{n}\\sum_{i=1}^n{a^{(i)}\\phi(x^{(i)})}=\\frac{1}{n}\\phi(X)a$$\n",
    "其中N维列向量 $a = [a_1,a_2,\\dots,a_N]^T$  \n",
    "又由于$\\Sigma v=\\lambda v$，我们可以得到：  \n",
    "$$\\frac{1}{n}\\phi(X)[\\phi(X)]^T\\phi(X)a=\\lambda\\phi(X)a$$\n",
    "两边都左乘矩阵$[\\phi(X)]^T$，得 5  \n",
    "$$\\frac{1}{n}[\\phi(X)]^T\\phi(X)[\\phi(X)]^T\\phi(X)a=\\lambda[\\phi(X)]^T\\phi(X)a$$\n",
    "$$\\Rightarrow\\frac{1}{n}[\\phi(X)]^T\\phi(X)a=\\lambda{a}$$\n",
    "定义矩阵 $K = [\\phi(X)]^T\\phi(X)$，则K为$N\\times N$的对称半正定矩阵，其i行j列的元素为$K_{ij}=[\\phi(x_i)]^T\\phi(x_j)$，将K代入得    \n",
    "$$\\frac{1}{n}Ka=\\lambda{a}$$\n",
    "K为相似（核）矩阵 5  \n",
    "### 问题：什么是半正定矩阵？\n",
    "> 线性代数概念  \n",
    "半正定矩阵是正定矩阵的推广。实对称矩阵A称为半正定的，如果二次型$X^TAX$半正定，即对于任意不为0的实列向量X，都有$X^TAX≥0$  \n",
    "正定矩阵有时会简称为正定阵。在线性代数中，正定矩阵的性质类似复数中的正实数。狭义定义：一个n阶的实对称矩阵M是正定的的条件是当且仅当对于所有的非零实系数向量z，都有$z^TMz> 0$  \n",
    "5    \n",
    "\n",
    "通过核技巧，使用核函数k以避免使用$\\phi$来精确计算样本集合x中样本对之间的点积，这样我们就无需对特征向量进行精确的计算  \n",
    "$$k(x^{(i)},x^{(j)})=\\phi(x^{(i)})^T\\phi(x^{(j)})$$\n",
    "通过核PCA，我们能够得到已经映射到各成分的样本，而不像标准PCA方法那样去构建一个转换矩阵  \n",
    "可以将核函数理解为：通过两个向量点积来度量向量间相似度的函数。常用的核函数有：  \n",
    "多项式核：$K(x^{(i)},x^{(j)})=(x^{(i)T}x^{(j)}+\\theta)^p$  5  \n",
    "其中，阈值$\\theta$和幂的值p需自行定义  \n",
    "双曲正切(sigmoid)核：$K(x^{(i)},x^{(j)})=thah(\\eta x^{(i)T}x^{(j)}+\\theta)$  \n",
    "径向基核函数（RBF）或者成为高斯核函数：$K(x^{(i)},x^{(j)})=exp\\left(-\\frac{\\|x^{(i)}-x^{(j)}\\|^2}{2\\sigma^2}\\right)$  \n",
    "也可以写作:$K(x^{(i)},x^{(j)})=exp(-\\gamma\\|x^{(i)}-x^{(j)}\\|^2)$  \n",
    "综合上述讨论，我们可以通过如下三个步骤来实现一个基于RBF核的PCA: 5  \n",
    "1. 为了计算核（相似）矩阵k,我们需要做如下计算：  \n",
    "$$K(x^{(i)},x^{(j)})=exp(-\\gamma\\|x^{(i)}-x^{(j)}\\|^2)$$\n",
    "我们需要计算任意两样本对之间的值：  \n",
    "$$K=\\left[\\begin{matrix}K(x^{(1)},x^{(1)})&K(x^{(1)},x^{(2)})&\\dots&K(x^{(1)},x^{(n)}) \\cr K(x^{(2)},x^{(1)})&K(x^{(2)},x^{(2)})&\\dots&K(x^{(2)},x^{(n)}) \\cr\n",
    "\\dots&\\dots&\\dots&\\dots \\cr \n",
    "K(x^{(n)},x^{(1)})&K(x^{(n)},x^{(2)})&\\dots&K(x^{(n)},x^{(n)})\\end{matrix}\\right]$$\n",
    "5  \n",
    "2. 通过如下公式，使核矩阵k更为聚集：  \n",
    "$$K'=K-1_nK-Kl_n+l_nKl_n$$\n",
    "其中，$l_n$是一个$n\\times n$维的矩阵（与核矩阵维度相同），其所有的值均为$\\frac{1}{n}$\n",
    "3. 将聚集后的核矩阵的特征值按照降序排列，选择前k个特征值所对应的特征向量。与标准PCA不同，这里的特征向量不是主成分轴，而是将样本映射到这些轴上  \n",
    "\n",
    "为什么要在第2步中对核矩阵进行聚集处理？因为我们并没有精确计算新的特征空间，不能确定新特征空间的中心在零点 5  \n",
    "## 5.3.2 使用Python实现核主成分分析  \n",
    "> 介绍了如何使用Python通过三个步骤来实现KPCA  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tscipy\tDistance computations (scipy.spatial.distance)\tpdist()\n",
    "python\tscipy\tDistance computations (scipy.spatial.distance)\tsquareform()\n",
    "python\tnumpy\tArray creation routines\tones()\n",
    "python\tscipy\tLinear algebra (scipy.linalg)\teigh()\n",
    "python\tnumpy\tArray manipulation routines\tcolumn_stack()\n",
    "'''\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from numpy import exp\n",
    "from scipy.linalg import eigh\n",
    "def rbf_kernel_pca(X, gamma, n_components):\n",
    "    \"\"\"\n",
    "    RBF kernel PCA implementation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: {NumPy ndarray}, shape = [n_samples, n_features]\n",
    "    \n",
    "    gamma: float\n",
    "        Tuning parameter of the RBF kernel\n",
    "    \n",
    "    n_components: int\n",
    "        Number of principal components to return\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    X_pc: {Numpy ndarray}, shape = [n_samples, k_features]\n",
    "        Projected dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    # 计算成对平方欧几里得距离 Calculate pairwise squared Euclidean distances\n",
    "    # in the NxN dimensional dataset.\n",
    "    sq_dists = pdist(X, 'sqeuclidean')\n",
    "    \n",
    "    # 将成对距离转换成一个方阵 Convert pairwise distances into a square matrix.\n",
    "    mat_sq_dists = squareform(sq_dists)\n",
    "    \n",
    "    # 计算对称核矩阵 Compute the symmetric kernel matrix\n",
    "    K = exp(-gamma * mat_sq_dists)\n",
    "    \n",
    "    # 聚集核心矩阵 Center the kernel matrix.\n",
    "    N = K.shape[0]\n",
    "    one_n = np.ones((N,N)) / N\n",
    "    K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)\n",
    "    \n",
    "    # 从聚集的核矩阵求特征对 Obtaining eigenpairs from the centered kernel matrix \n",
    "    # numpy.eigh returns them in sorted order\n",
    "    eigvals, eigvecs = eigh(K)\n",
    "    \n",
    "    # 收集前k个特征向量 Collect the top k eigenvectors(projected samples)\n",
    "    X_pc = np.column_stack([eigvecs[:, -i]\n",
    "                           for i in range(1, n_components +1)])\n",
    "    return X_pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采用RBF核函数实现的PCA进行降维时必须指定先验参数r，需要通过实验来找到一个合适的r值  \n",
    "### 示例一：分离半月形数据   \n",
    "\n",
    "### 问题：如何理解pdist?\n",
    "> D = pdist(X)    计算 X 中各对行向量的相互距离(X是一个m-by-n的矩阵). 这里 D 要特别注意，D 是一个长为m(m–1)/2的行向量.可以这样理解 D 的生成：首先生成一个 X 的距离方阵，由于该方阵是对称的，令对角线上的元素为0，所以取此方阵的下三角元素，按照Matlab中矩阵的按列存储原则，此下三角各元素的索引排列即为(2,1), (3,1), ..., (m,1), (3,2), ..., (m,2), ..., (m,m–1). 5  \n",
    "\n",
    "### 问题：如何理解squareform？\n",
    "> 用来把一个向量格式的距离向量转换成一个方阵格式的距离矩阵，反之亦然。\n",
    "首先输入如果是矩阵的话必须是距离矩阵，距离矩阵的特点是 \n",
    "    1. d*d的对称矩阵，这里d表示点的个数\n",
    "    2. 对称矩阵的主对角线都是0；\n",
    "另外，如果输入的是距离向量的话，必须满足d * (d-1) / 2  \n",
    "5  \n",
    "\n",
    "### 问题：如何理解eigh?\n",
    "> 求解复数矩阵-Hermitian矩阵或实对称矩阵的标准或广义特征值问题;返回正半定矩阵的特征值和特征向量 5  \n",
    "\n",
    "我们先实现非线性示例数据集，以两个半月形状表示    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tDatasets\tmake_moons()\n",
    "5\n",
    "'''\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=100, random_state=123)\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1],\n",
    "           color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1],\n",
    "           color='blue', marker='o', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这两个半月形是线性不可分的，我们的目标是通过核PCA将这两个半月形数据展开，首先我们尝试观察标准PCA映射后的效果   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tMatrix Decomposition\tPCA()\n",
    "python\tmatplotlib\tPyplot function overview\tsubplot()\n",
    "python\tmatplotlib\taxes\tscatter()\n",
    "python\tmatplotlib\taxes\tset_xlabel()\n",
    "python\tmatplotlib\taxes\tset_ylabel()\n",
    "python\tmatplotlib\taxes\tset_ylim()\n",
    "python\tmatplotlib\taxes\tset_yticks()\n",
    "'''\n",
    "scikit_pca = PCA(n_components=2)\n",
    "X_spca = scikit_pca.fit_transform(X)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7,3))\n",
    "ax[0].scatter(X_spca[y==0, 0], X_spca[y==0, 1],\n",
    "             color='red', marker='^', alpha=0.5)\n",
    "ax[0].scatter(X_spca[y==1, 0], X_spca[y==1, 1],\n",
    "             color='blue', marker='o', alpha=0.5)\n",
    "\n",
    "# 请注意，当我们绘制第一主成分时，为了更好地展示类间重叠\n",
    "# 我们分别将三角形和圆形代表的样本向上或向下做了轻微调整（0.02）\n",
    "\n",
    "ax[1].scatter(X_spca[y==0, 0], np.zeros((50, 1))+0.02,\n",
    "             color='red', marker='^', alpha=0.5)\n",
    "ax[1].scatter(X_spca[y==1, 0], np.zeros((50, 1))-0.02,\n",
    "             color='blue', marker='o', alpha=0.5)\n",
    "ax[0].set_xlabel('PC1')\n",
    "ax[0].set_ylabel('PC2')\n",
    "ax[1].set_ylim([-1,1])\n",
    "ax[1].set_yticks([])\n",
    "ax[1].set_xlabel('PC1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，PCA是无监督，过程中是未使用类标信息的，出于增强可视化效果的考虑，我们才在此使用了三角形和圆形符号用于区别类标  \n",
    "现在我们将使用前一小节实现的核PCA函数:rbf_kernel_pca 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tmatplotlib\tticker\tFormatStrFormatter()\n",
    "python\tmatplotlib\taxis\tset_major_formatter()\n",
    "'''\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "X_kpca = rbf_kernel_pca(X, gamma=15, n_components=2)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(7,3))\n",
    "ax[0].scatter(X_kpca[y==0, 0], X_kpca[y==0, 1],\n",
    "             color='red', marker='^', alpha=0.5)\n",
    "ax[0].scatter(X_kpca[y==1, 0], X_kpca[y==1, 1],\n",
    "             color='blue', marker='o', alpha=0.5)\n",
    "ax[1].scatter(X_kpca[y==0, 0], np.zeros((50, 1))+0.02,\n",
    "             color='red', marker='^', alpha=0.5)\n",
    "ax[1].scatter(X_kpca[y==1, 0], np.zeros((50, 1))-0.02,\n",
    "             color='blue', marker='o', alpha=0.5)\n",
    "ax[0].set_xlabel('PC1')\n",
    "ax[0].set_ylabel('PC2')\n",
    "ax[1].set_ylim([-1,1])\n",
    "ax[1].set_yticks([])\n",
    "ax[1].set_xlabel('PC1')\n",
    "ax[0].xaxis.set_major_formatter(FormatStrFormatter('%0.1f'))\n",
    "ax[1].xaxis.set_major_formatter(FormatStrFormatter('%0.1f'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，两个类别此时是线性可分的，这使得转换后的数据适合作为线性分类器的训练数据集  \n",
    "不过，对于可调整参数$\\gamma$，没有一个通用的值使其适用于不同的数据集  \n",
    "### 示例二：分离同心圆\n",
    "我们再看一下另外一个关于非线性问题的例子-同心圆  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tsklearn\tDatasets\tmake_circles()\n",
    "'''\n",
    "from sklearn.datasets import make_circles\n",
    "X, y = make_circles(n_samples=1000,\n",
    "                   random_state=123, noise=0.1, factor=0.2)\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1],\n",
    "           color='red', marker='^', alpha=0.5)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1],\n",
    "           color='blue', marker='^', alpha=0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
