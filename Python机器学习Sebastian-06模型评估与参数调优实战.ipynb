{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本章简介\n",
    "本章，我们将通过对算法进行调优来构建性能良好的机器学习模型，并对模型性能进行评估：\n",
    "+ 模型性能的无偏估计\n",
    "+ 处理机器学习中的常见问题\n",
    "+ 机器学习模型调优\n",
    "+ 使用不同的性能指标评估预测模型 5\n",
    "\n",
    "# 6.1 基于流水线的工作流\n",
    "> 介绍了如何使用sklearn中的工具类pipline使得工作更加高效  \n",
    "\n",
    "sklearn中的pipline类使得我们可以拟合出包含任意多个处理步骤的模型，并将模型用于新数据的预测 5  \n",
    "## 6.1.1 加载威斯康星乳腺癌数据集\n",
    "> 介绍了如何加载威斯康星乳腺癌数据集并进行预处理  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步：加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tDatasets\tload_breast_cancer()\n",
    "python\tsklearn\tDatasets\tload_*() as_frame\n",
    "python\tsklearn\tDatasets\tload_*() frame\n",
    "'''\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "bc = load_breast_cancer(as_frame=True)\n",
    "df = bc.frame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二步：对类标编码，此时恶性肿瘤和良性肿瘤分别被标识为类1和类0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "'''\n",
    "import numpy as np\n",
    "X = df.iloc[:, :-2].values\n",
    "y = df.iloc[:, -1].values\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三步：我们将数据集划分为训练数据集(80%)和单独的测试数据集(20%)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tsklearn\tModel Selection\ttrain_test_split()\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.2 在流水线中集成数据转换及评估操作\n",
    "> 介绍了如何通过pipeline实现预处理、训练等一系列操作以及pipeline的工作方式  \n",
    "\n",
    "我们需要对数据特征列做标准化处理，此外，我们还想通过第5章介绍过的主成分分析降维到二维子空间，最后我们使用逻辑斯蒂回归模型分析数据  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tPreprocessing and Normalization\tStandardScaler()\n",
    "python\tsklearn\tMatrix Decomposition\tPCA()\n",
    "python\tsklearn\tpipeline\tPipeline()\n",
    "python\tsklearn\tPipeline\t*.fit()\n",
    "python\tsklearn\tPipeline\t*.score()\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe_lr = Pipeline([('scl', StandardScaler()),\n",
    "                   ('pca', PCA(n_components=2)),\n",
    "                   ('clf', LogisticRegression(random_state=1))])\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "print('Test Accuracy: {:.3f}'.format(pipe_lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline对象的输入可以是元组的序列，每个元组作为流水线的一个步骤，元组的第一个值必须是字符串，作为步骤的标识，可以用来访问具体的步骤元素，第二个值则为sklearn中的转换器、评估器  \n",
    "当流水线上执行fit时，每个步骤会依次执行fit和transform操作，经过转换的数据会传递给流水线下一个对象 5   \n",
    "流水线的工作方式可用下图来描述  \n",
    "![6-1](../syn_pic/py_machine_learning/6-1.png)\n",
    "5  \n",
    "# 6.2 使用k折交叉验证评估模型性能 \n",
    "> 介绍了用于模型性能评估的两种技术：holdout交叉验证和k折交叉验证的实现原理和方法  \n",
    "\n",
    "如果一个模型过于简单会面临欠拟合（高偏差）问题，而模型过于复杂则会导致过拟合（高方差）问题。为了在偏差和方差之间找到折中的方案，我们需要对模型进行评估 5  \n",
    "## 6.2.1 holdout方法\n",
    "> 介绍了为何使用holdout交叉验证和其实现原理\n",
    "\n",
    "为进一步提高模型在预测未知数据上的性能，我们还要进行模型选择过程，所谓模型选择过程：就是指针对给定分类问题我们调整参数并比较以寻求最优值  \n",
    "在模型选择过程中，会产生一个问题，如果我们不断重复使用相同的测试数据，它们会成为训练数据的一部分，导致模型更易于过拟合  \n",
    "所以更好的方法将数据划分为三个部分：训练数据集、验证数据集和测试数据集。其中训练数据集用于不同模型的拟合，验证数据集用于模型选择的标准  \n",
    "下图介绍holdout交叉验证的概念 5  \n",
    "![6-2](../syn_pic/py_machine_learning/6-2.png)\n",
    "holdout方法的一个缺点在于：模型性能的评估对训练数据集划分为训练及验证子集的方法是敏感的，评价的结果会随样本的不同而发生变化 5  \n",
    "## 6.2.2 k折交叉验证\n",
    "> 介绍了k折交叉验证、分层k折交叉验证技术的实现原理和使用方法\n",
    "\n",
    "k折交叉验证克服了holdout方法的缺点，对数据划分方法的敏感性较低，具有更好的鲁棒性  \n",
    "其方法是不重复地随机将训练数据集划分为k个，其中k-1个用于模型的而训练，剩余的1个用于测试。随后重复此过程k次，我们就得到k个模型及对模型性能的评价  \n",
    "基于这些独立且不同的数据子集上得到的模型性能评价结果，我们可以计算出其平均性能  \n",
    "一旦找到了满意的超参值，我们就可以在全部训练集上重新训练模型，并使用独立测试集对模型做出最终评价 5  \n",
    "由于k折交叉验证使用了无重复抽样，使得每个样本点只有一次被划入训练集或测试集的机会，与holdout方法相比，这将使得模型性能的评估具有较小的方差  \n",
    "### 问题：为何无重复抽样技术可以使得模型性能评估具有较小方差?\n",
    "> 待定  \n",
    "\n",
    "下图总结了k折交叉验证的相关概念，其中k=10。$E_i$是第i次迭代的性能评价指标  \n",
    "![6-3](../syn_pic/py_machine_learning/6-3.png)\n",
    "k折交叉验证中的k标准值为10，但如果训练数据集相对较小，有必要加大k的值，这样会有更多的数据用于模型的训练，这样利用评估结果平均值对模型泛化性能进行评价时可以得到更小的偏差  \n",
    "但是由于k值得增加导致交叉验证算法运行时间延长，而且各训练块间高度相似，也会导致评价结果方差较高。所以数据集较大，则可以选择较小得k值，如k=5 5   \n",
    "k折交叉验证得一个特例就是留一(LOO)交叉验证法。在LOO中，我们将数据子集划分的数量等同于样本数(k=n)，这样每次只有一个样本用于测试，主要用于数据集非常小时的验证  \n",
    "分层k折交叉验证对标准k折交叉验证做了稍许改进，它可以获得偏差和方差都较低的评估结果，特别时类别比例相差较大时  \n",
    "在分k折交叉验证中，类别比例在每个分块中得以保持，下面通过sklearn中的stratifiedKFold迭代器来演示  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tModel Selection\tStratifiedKFold()\n",
    "python\tsklearn\tModel Selection\t*.split()\n",
    "python\tnumpy\tStatistics\tbincount()\n",
    "'''\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=10)\n",
    "scores = []\n",
    "for k, (train, test) in enumerate(kfold.split(X_train, y_train)):\n",
    "    pipe_lr.fit(X_train[train], y_train[train])\n",
    "    score = pipe_lr.score(X_train[test], y_train[test])\n",
    "    scores.append(score)\n",
    "    print('Fold: {}, Class dist.：{}， Acc：{:.3f}'.format(k+1, \n",
    "                                                        np.bincount(y_train[train]), score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tnumpy\tStatistics\tmean()\n",
    "python\tnumpy\tStatistics\tstd()\n",
    "'''\n",
    "print('CV accuracy: {:.3f} +/- {:.3f}'.format(np.mean(scores),\n",
    "                                             np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们test索引计算模型的准确率，将其存储在score列表中，用于计算平均准确率及性能评估标准差  \n",
    "另外sklearn也同样实现了k折交叉验证评分的计算  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tModel Selection\tcross_val_score()\n",
    "'''\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(estimator=pipe_lr,\n",
    "                        X=X_train,\n",
    "                        y=y_train,\n",
    "                        cv=10,\n",
    "                        n_jobs=1)\n",
    "print('CV accuracy scores:{}'.format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CV accuracy: {:.3f} +/- {:.3f}'.format(np.mean(scores),\n",
    "                                             np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_val_score通过设定n_jobs可以支持多CPU并行运算 5  \n",
    "# 6.3 通过学习及验证曲线来调试算法\n",
    "> 本章介绍了如何使用两个有助于提高学习算法性能的判定工具：学习曲线与验证曲线 \n",
    "\n",
    "学习曲线和验证曲线可以用来判定学习算法是否面临过拟合（高方差）或欠拟合（高偏差）问题 5  \n",
    "## 6.3.1 使用学习曲线判定偏差和方差问题  \n",
    "> 介绍了如何运用学习曲线来判定模型是否拟合良好，以及是否应该收集更多数据  \n",
    "\n",
    "通过将模型的训练及准确性验证看作是训练数据集大小的函数，并绘制其图像，我们可以很容易看出模型是面临高方差还是高偏差的问题  \n",
    "![6-4](../syn_pic/py_machine_learning/6-4.png)\n",
    "左上方显示的是一个高偏差模型，解决此问题的常用方法是增加模型中参数的数量，例如，收集或构建额外特征，或者降低类似于SVM等模型的正则化程度  \n",
    "右上方图像中的模型面临高方差的问题，表明训练准确度与交叉验证准确度之间有很大差距 5  \n",
    "针对此类过拟合问题，我们可以收集更多的训练数据或者降低模型的复杂度，如正则化参数。或用特征抽取来降低特征的数量  \n",
    "需要注意：收集更多数据不适用于所有问题，例如训练数据中噪声过多，或者模型本身接近最优  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tsklearn\tPipeline\tPipeline()\n",
    "python\tsklearn\tModel Selection\tlearning_curve()\n",
    "python\tmatplotlib\tPyplot function overview\tfill_between()\n",
    "python\tmatplotlib\tPyplot function overview\tgrid()\n",
    "b\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "pipe_lr = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('clf', LogisticRegression(\n",
    "        penalty='l2', random_state=0))])\n",
    "train_sizes, train_scores, test_scores =\\\n",
    "    learning_curve(estimator=pipe_lr,\n",
    "                  X=X_train,\n",
    "                  y=y_train,\n",
    "                  train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                  cv=10,\n",
    "                  n_jobs=1)\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "plt.plot(train_sizes, train_mean,\n",
    "        color='blue', marker='o',\n",
    "        markersize=5,\n",
    "        label='training accuracy')\n",
    "plt.fill_between(train_sizes,\n",
    "                train_mean + train_std,\n",
    "                train_mean - train_std,\n",
    "                alpha=0.15, color='blue')\n",
    "plt.plot(train_sizes, test_mean,\n",
    "        color='green', linestyle='--',\n",
    "        marker='s',markersize=5,\n",
    "        label='validation accuracy')\n",
    "plt.fill_between(train_sizes,\n",
    "                test_mean + test_std,\n",
    "                test_mean - test_std,\n",
    "                alpha=0.15, color='green')\n",
    "plt.grid()\n",
    "plt.xlabel('Number of training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过learning_curve函数的train_sizes参数，我们可以控制用于生成学习曲线的样本的决定或相对数量。learning_curve函数使用分层K折交叉验证来计算交叉验证准确率  \n",
    "我们通过fill_between函数加入了平均准确率标准差的信息，用以标识评价结果的标准差  \n",
    "由于训练准确率曲线稍高于验证准确率，意味着模型对训练数据有轻微的过拟合，可以通过增大正则化来解决 5  \n",
    "## 6.3.2 通过验证曲线来判定过拟合与欠拟合  \n",
    "> 介绍了如何运用验证曲线来判定模型是否拟合良好，以及是否需要调整模型参数  \n",
    "\n",
    "验证曲线与学习曲线相似，不过绘制的不是样本大小与训练准确率、测试准确率之间的函数关系，而是准确率与模型参数之间的关系  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tModel Selection\tvalidation_curve()\n",
    "'''\n",
    "from sklearn.model_selection import validation_curve\n",
    "param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "train_scores, test_scores =\\\n",
    "    validation_curve(estimator=pipe_lr,\n",
    "                  X=X_train,\n",
    "                  y=y_train,\n",
    "                  param_name='clf__C',\n",
    "                  param_range=param_range,\n",
    "                  cv=10)\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "plt.plot(param_range, train_mean,\n",
    "        color='blue', marker='o',\n",
    "        markersize=5,\n",
    "        label='training accuracy')\n",
    "plt.fill_between(param_range,\n",
    "                train_mean + train_std,\n",
    "                train_mean - train_std,\n",
    "                alpha=0.15, color='blue')\n",
    "plt.plot(param_range, test_mean,\n",
    "        color='green', linestyle='--',\n",
    "        marker='s',markersize=5,\n",
    "        label='validation accuracy')\n",
    "plt.fill_between(param_range,\n",
    "                test_mean + test_std,\n",
    "                test_mean - test_std,\n",
    "                alpha=0.15, color='green')\n",
    "plt.grid()\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Parameter C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([0.8, 1.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在validation_curve函数内，我们可以指定想要验证的参数。例如，需要验证参数C，即定义在流水线中逻辑斯蒂回归分类器的正则化参数，我们将其记为'clf_C'  \n",
    "我们可以看到，如果加大正则化强度（较小的C值），会导致模型轻微的欠拟合;如果增加C的值，这意味着降低正则化强度，模型会趋向于过拟合，最优点在C=0.1附近 5  \n",
    "# 6.4 使用网格搜索调优机器学习模型\n",
    "> 介绍了调优模型超参的技术-网格搜索，以及在不同算法中做出选择的技术-嵌套交叉验证的概念和使用方法  \n",
    "\n",
    "机器学习中的两类参数：通过训练数据学习得到的参数，以及学习算法中需要单独进行优化的参数  \n",
    "后者称为调优参数，也称为超参，如LR中的正则化系数或者决策树的深度参数 5  \n",
    "## 6.4.1 使用网格搜索调优超参\n",
    "> 介绍了网格搜索使用方法\n",
    "\n",
    "网格搜索法的原理是通过我指定的不同超参列表进行暴力穷举搜索，并计算评估每个组合对模型性能的影响  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tModel Selection\tGridSearchCV()\n",
    "python\tsklearn\tModel Selection\t*.best_score_\n",
    "'''\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "pipe_svc = Pipeline([('scl', StandardScaler()),\n",
    "                    ('clf', SVC(random_state=1))])\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "param_grid = [{'clf__C': param_range,\n",
    "              'clf__kernel': ['linear']},\n",
    "             {'clf__C': param_range,\n",
    "              'clf__gamma': param_range,\n",
    "              'clf__kernel': ['rbf']}]\n",
    "gs = GridSearchCV(estimator=pipe_svc,\n",
    "                 param_grid=param_grid,\n",
    "                 scoring='accuracy',\n",
    "                 cv=10,\n",
    "                 n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tModel Selection\t*.best_params_\n",
    "'''\n",
    "print(gs.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
