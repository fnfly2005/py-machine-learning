{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本章简介\n",
    "> 本章介绍了集成学习技术的实现和使用方法：多数投票、bagging、boosting\n",
    "\n",
    "在本章我们将构建构建一组分类器的集合，使得整体分类效果优于任意一个单独的分类器  \n",
    "+ 基于多数投票的预测\n",
    "+ 通过对训练数据集的重复抽样和随机组合降低模型的过拟合\n",
    "+ 通过弱学习机在误分类数据上的学习构建性能更好的模型 5  \n",
    "\n",
    "# 7.1 集成学习\n",
    "> 介绍了集成学习的实现原理-通过不同的分类器组成一个元分类器\n",
    "\n",
    "集成方法的目标是：将不同的分类器组合成为一个元分类器，与包含于其中的单个分类器相比，元分类器具有更好的泛化性能  \n",
    "本章介绍的集成方法都试用了多数投票原则：二类别分类情形下将得票率超过50%的结果作为类标  \n",
    "下图介绍了集成10个分类器时，多数及简单多数票法表决的概念，其中每个单独的符号分别代表一个类标  \n",
    "![7-1 5](../syn_pic/py_machine_learning/7-1.png)\n",
    "基于训练集，首先训练m个不同的成员分类器，在多数投票原则下，可集成不同的分类算法，也可使用相同的成员分类器拟合不同的训练子集  \n",
    "下图介绍了使用多数投票原则的通用集成方法的概念  \n",
    "![7-2](../syn_pic/py_machine_learning/7-2.png)\n",
    "我们汇总所有分类器$C_j$的预测类标，选出得票率最高的类标$\\hat{y}$\n",
    "$$\\hat{y}=mode\\{C_1(x),C_2(x),\\dots,C_m(x)\\}$$\n",
    "5  \n",
    "在二类别分类中，假定class1=-1、class2=+1，我们可以将多数投票预测表示为：  \n",
    "$$C(x)=sign\\left[\\sum_j^m{C_j(x)}\\right]=\\begin{cases} 1&若\\sum_i{C_j(x)\\ge0} \\cr -1&其他 \\end{cases}$$\n",
    "我们假定二类别分类中的n个成员分类器都有相同的出错率$\\varepsilon$，假定每个分类器都是独立的，出错率之间是不相关的  \n",
    "基于这些假设，我们可以将成员分类器集成后的出错概率简单地表示为二项分布的概率密度函数 5  \n",
    "$$P(y\\ge k)=\\sum_k^n{\\left\\langle\\begin{matrix}n \\cr k\\end{matrix}\\right\\rangle}\\varepsilon^k(1-\\varepsilon)^{n-k}=\\varepsilon_{ensemble}$$\n",
    "其中，$\\left\\langle\\begin{matrix}n \\cr k\\end{matrix}\\right\\rangle$是n选k组合的二项式系数  \n",
    "再来看一个更具体的例子：假定使用11个分类器(n=11)，单个分类器出错率为0.25  \n",
    "$$P(y\\ge k)=\\sum_{k=6}^n{\\left\\langle\\begin{matrix}11 \\cr k\\end{matrix}\\right\\rangle}0.25^k(1-\\varepsilon)^{11-k}=0.034$$\n",
    "5  \n",
    "在满足所有假设的条件下，集成后的出错率远小于单个分类器的出错率  \n",
    "我们用户Python比较一下成员分类器在不同出错率情况下与集成分类器出错率的差异  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tscipy\tSpecial functions (scipy.special)\tcomb()\n",
    "python\tmath — Mathematical functions\tNumber-theoretic and representation functions\tceil()\n",
    "python\tBuilt-in Functions\tsum()\tsum()\n",
    "'''\n",
    "from scipy.special import comb\n",
    "import math\n",
    "def ensemble_error(n_classifier, error):\n",
    "    k_start = math.ceil(n_classifier / 2.0)\n",
    "    probs = [comb(n_classifier, k) * error**k *\n",
    "            (1-error)**(n_classifier - k)\n",
    "            for k in range(k_start, n_classifier + 1)]\n",
    "    return sum(probs)\n",
    "ensemble_error(n_classifier=11, error=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在成员分类器出错率介于0.0到1.0范围内，我们可以以函数曲线的形式绘制出它们之间的关系  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "'''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "error_range = np.arange(0.0, 1.01, 0.01)\n",
    "ens_errors = [ensemble_error(n_classifier=11, error=error)\n",
    "             for error in error_range]\n",
    "plt.plot(error_range, ens_errors,\n",
    "        label='Ensemble error',\n",
    "        linewidth=2)\n",
    "plt.plot(error_range, error_range,\n",
    "        linestyle='--', label='Base error',\n",
    "        linewidth=2)\n",
    "plt.xlabel('Base error')\n",
    "plt.ylabel('Base/Ensemble error')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从结果可见：当成员分类器出错率低于随机猜测时($\\varepsilon\\gt0.5$)，集成分类器的出错率要低于单个分类器 5  \n",
    "# 7.2 实现一个简单的多数投票分类器\n",
    "> 介绍了如何实现一个多数投票分类器及其效果\n",
    "\n",
    "我们的目标是构建一个更加强大的元分类器，以在特定的数据集上平衡单个分类器的弱点。可以将加权多数投票记为  \n",
    "$$\\hat{y}=arg \\max \\limits_i\\sum_{j=1}^m{w_j\\chi_A(C_j(x)=i)}$$\n",
    "其中,$w_j$是成员分类器$C_j$对应的权重，$\\hat{y}$为集成分类器的预测类标，$\\chi_A$为特征函数$[C_j(x)=i\\in A]$，A为类别的集合。如果权重相等，可简化为 5  \n",
    "$$\\hat{y}=mode\\{C_1(x),C_2(x),\\dots,C_m(x)\\}$$\n",
    "为了更好地理解权重的含义，我们来看几个例子。假定三个成员分类器$C_j(i\\in\\{1,2,3\\})$,分别用它们来预测样本x的类标：  \n",
    "$$C_1(x)\\rightarrow0,C_2(x)\\rightarrow0,C_3(x)\\rightarrow1$$\n",
    "$$\\hat{y}=mode\\{0,0,1\\}=0$$\n",
    "5  \n",
    "我们将权重0.6赋值给$C_3$，而$C_1$和$C_2$均为0.2，有：  \n",
    "$$\\hat{y}=arg \\max \\limits_i\\sum_{j=1}^m{w_j\\chi_A(C_j(x)=i)}$$\n",
    "$$=arg \\max \\limits_i\\sum_{j=1}^m{w_j\\chi_A(C_j(x)=i)}$$\n",
    "更直观地，由于$3\\times 0.2=0.6$，可以认为分类器$C_3$的一次预测权重相当于分类器$C_1$或$C_2$的三次预测权重之和，我们可以写作：  \n",
    "$$\\hat{y}=mode\\{0,0,1,1,1\\}=1$$\n",
    "5  \n",
    "我们可以使用numpy的argmax和bincount函数来实现加权投票  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tSorting, searching, and counting\targmax()\n",
    "python\tnumpy\tStatistics\tbincount()\n",
    "'''\n",
    "np.argmax(np.bincount([0, 0, 1], weights=[0.2, 0.2, 0.6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果集成分类器事先得到良好的修正，那么在多数投票中使用预测类别的概率来替代类标会非常有用。使用类别概率进行预测的多数投票修改版本可记为：  \n",
    "$$\\hat{y}=arg \\max \\limits_i\\sum_{j=1}^m{w_jp_{ij}}$$\n",
    "其中，$p_{ij}$是第j个分类器预测样本类标为i的概率 5  \n",
    "假设我们面临一个二类别分类问题，其类标为$i\\in\\{0,1\\}$，下面集成这三个分类器$C_j(j\\in\\{1,2,3\\})$。假定分类器$C_j$针对某一样本x按照如下概率返回类标的预测结果：  \n",
    "$$C_1(x)\\rightarrow[0.9,0.1],C_2(x)\\rightarrow[0.8,0.2],C_3(x)\\rightarrow[0.4,0.6]$$\n",
    "我们可以按照如下方式计算所属类别的概率：  \n",
    "$$p(i_0|x)=0.2\\times 0.9+0.2\\times0.8+0.6\\times0.4=0.58$$\n",
    "$$p(i_i|x)=0.2\\times 0.1+0.2\\times0.2+0.6\\times0.06=0.42$$\n",
    "5  \n",
    "$$\\hat{y}=arg \\max\\limits_i[p(i_0|x),p(i_1|x)]=0$$\n",
    "为实现基于类别预测概率的加权多数投票，我们可以再次使用Numpy中的numpy.average和np.argmax方法  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tStatistics\taverage()b\n",
    "'''\n",
    "ex = np.array([[0.9, 0.1],\n",
    "             [0.8, 0.2],\n",
    "             [0.4, 0.6]])\n",
    "p = np.average(ex, axis=0, weights=[0.2, 0.2, 0.6])\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们来实现MajorityVoteClassifier类 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "'''\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import six\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import _name_estimators\n",
    "import operator\n",
    "\n",
    "class MajorityVoteClassifier(BaseEstimator,\n",
    "                            ClassifierMixin):\n",
    "    \"\"\" A majority vote ensemble classifier\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    classifiers : array-like, shape = [n_classifiers]\n",
    "     Different classifiers for the ensemble\n",
    "     \n",
    "    vote : str, {'classlabel', 'probability'}\n",
    "     Default: 'classlabel'\n",
    "     If 'classlabel' the prediction is based on \n",
    "     the argmax of class labels. Else if\n",
    "     'probability', the argmax of the sum of\n",
    "     probabilities is used to predict the class label\n",
    "     (recommended for calibrated classifiers).\n",
    "     \n",
    "    weights : array-like, shape = [n_classifiers]\n",
    "     Optional, default: None\n",
    "     If a list of `int` or `float` values are\n",
    "    provided, the classifiers are weighted by\n",
    "    importance; Uses uniform weights if `weights=None`.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, classifiers,\n",
    "                vote='classlabel', weights=None):\n",
    "        self.classifiers = classifiers\n",
    "        self.named_classifiers = {key: value for\n",
    "                                 key, value in \n",
    "                                 _name_estimators(classifiers)}\n",
    "        self.vote = vote\n",
    "        self.weights = weights\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit classifiers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matix}，\n",
    "            shape = [n_samples, n_features]\n",
    "            Matrux of training samples.\n",
    "        \n",
    "        y : array-like, shape = [n_samples]\n",
    "            Vector of target class lables.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        # Use LabelEncoder to ensure class labels start\n",
    "        # with 0, which is important for np.argmax\n",
    "        # call in self.predict\n",
    "        self.lablenc_ = LabelEncoder()\n",
    "        self.lablenc_.fit(y)\n",
    "        self.classes_ = self.lablenc_.classes_\n",
    "        self.classifiers_ = []\n",
    "        for clf in self.classifiers:\n",
    "            fitted_clf = clone(clf).fit(X,\n",
    "                                       self.lablenc_.transform(y))\n",
    "            self.classifiers_.append(fitted_clf)\n",
    "        return self"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
