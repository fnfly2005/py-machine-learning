{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章将涵盖如下内容：\n",
    "+ 培养对机器学习算法的直观认识\n",
    "+ 使用pandas、numpy和matplotlib读取、处理和可视化数据\n",
    "+ 使用python实现线性分类算法 \n",
    "\n",
    "5\n",
    "# 2.1 人造神经元-早期机器学习概览\n",
    "为了理解大脑的工作原理以设计人工智能系统，沃伦·麦卡洛可与沃尔特·皮茨在1943年提出了第一个脑神经元的抽象模型，也称作麦卡洛可-皮茨神经元（MCP），神经元是大脑中互相连接的神经细胞，它可以处理和传递化学和电信号，如下图所示  \n",
    "![2-1](../syn_pic/py_machine_learning/2-1.png)\n",
    "麦卡洛可和皮茨将神经细胞描述为一个具备二进制输出的逻辑门  \n",
    "几年后，弗兰克·罗森布拉特基于MCP神经元模型提出了第一个感知器学习法则。在规则中提出了一个自学习算法，此算法可以自动通过优化得到权重系数，此系数与输入值的乘积决定了神经元是否被激活  \n",
    "我们把问题看作一个二值分类的任务，把两类分别记为1(正类别)和-1(负类别) 5  \n",
    "我们可以定义一个激励函数$\\varphi (z)$,它以特定的输入值x与相应的权值向量w的线性组合作为输入，其中，z也称作净输入($z=w_1x_1+\\dots+w_mx_m$):  \n",
    "$$w=\\left[\\begin{matrix}w_1 \\cr \\dots \\cr w_m\\end{matrix}\\right],x=\\left[\\begin{matrix}x_1 \\cr \\dots \\cr x_m\\end{matrix}\\right]$$  \n",
    "此时，对于一个特定样本$x^{(i)}$的激励，也就是$\\varphi(z)$的输出，如果其值大于预设的阈值$\\theta$，我们将其划分到1类，否则为-1类  \n",
    "在感知器算法中，激励函数$\\varphi (z)$是一个简单的分段函数  \n",
    "$$\\phi{(z)}=\\begin{cases}1&若z\\ge\\theta \\cr -1&其他\\end{cases}$$\n",
    "5  \n",
    "我们可以把阈值$\\theta$移到等式的左边，并增加一个初始项权重记为$w_0=-\\theta$且设$x_0=1$，这样我们就可以把z写成一个更加紧凑的形式：  \n",
    "$$z=w_0x_0+w_1x_1+\\dots+w_mx_m=w^Tx,\\theta(z)=\\begin{cases}1&若z\\ge\\theta \\cr -1&其他\\end{cases}$$\n",
    "注意：下面的小节中，使用向量点积来表示x和w乘积的和，而上标T则表示转置，它使得行向量和列向量之间能够互相转换  \n",
    "下图中，左图说明了感知器模型的激励函数如何将输入$z=w^Tx$转换到二值输出(-1或1),右图说明了感知器模型如何将两个可区分类别进行线性区分  \n",
    "![2-2](../syn_pic/py_machine_learning/2-2.png)\n",
    "5  \n",
    "MCP神经元和罗森布拉特阈值感知器的理念就是，通过模拟的方式还原大脑中单个神经元的工作方式：它是否被激活。最初的规则可总结为如下几步：  \n",
    "1. 将权重初始化为零或一个极小的随机数  \n",
    "2. 迭代所有训练样本$x^{(i)}$，执行如下操作：\n",
    "    1. 计算输出值$\\hat{y}$\n",
    "    2. 更新权重\n",
    "    \n",
    "5  \n",
    "这里的输出值是指通过前面定义的单位阶跃函数预测得出的类标，而每次对权重向量中每一权重w的更新方式为：  \n",
    "$$w_j: = w_j + \\Delta{w_j}$$\n",
    "对于用于更新权重$w_j$的值$\\Delta w_j$, 可通过感知器学习规则计算获得：\n",
    "$$\\Delta{w_j} = \\eta(y^{(i)}-\\hat{y}^{(i)})x_j^{(i)}$$\n",
    "其中，$\\eta$为学习速率（一个介于0.0到1.0之间的常数）,$y^{(i)}$为第i个样本的真实类标，$\\hat{y}^{(i)}$为预测得到的类标 5  \n",
    "需特别注意，权重向量中的所有权重值是同时更新的，这意味着在所有的权重$\\Delta{w_j}$更新前，我们无法重新计算$\\hat{y}^{(i)}$  \n",
    "具体地，对于一个二维数据集，可通过下式进行更新：  \n",
    "$$\\Delta{w_0} = \\eta(y^{(i)} - output^{(i)})$$\n",
    "$$\\Delta{w_1} = \\eta(y^{(i)} - output^{(i)})x_1^{(i)}$$\n",
    "$$\\Delta{w_2} = \\eta(y^{(i)} - output^{(i)})x_2^{(i)}$$\n",
    "5  \n",
    "对于如下式所示的两种场景，若感知器对类标的预测正确，权重可不做更新：  \n",
    "$$\\Delta{w_j} = \\eta(-1^{(i)} - (-1^{(i)}))x_j^{(i)} = 0$$\n",
    "$$\\Delta{w_j} = \\eta(1^{(i)} - 1^{(i)})x_j^{(i)} = 0$$\n",
    "但是，在类标预测错误的情况下，权重的值会分别趋向于正类别或者负类别的方向：  \n",
    "$$\\Delta{w_j} = \\eta(1^{(i)} - (-1^{(i)}))x_j^{(i)} = \\eta(2)x_j^{(i)}$$\n",
    "$$\\Delta{w_j} = \\eta(-1^{(i)} - 1^{(i)})x_j^{(i)} = \\eta(-1)x_j^{(i)}$$\n",
    "5  \n",
    "为了对乘法因子x_j^{(i)}有个更直观的认识，我们看另一个简单的例子，其中  \n",
    "$$\\hat{y}^{(i)}=+1，y^{(i)}=-1，\\eta=1$$  \n",
    "假定$x_j^{(i)} = 0.5$, 且模型将此样本错误地分到了-1类别内  \n",
    "在此情况下，我们应将相应的权值增1，以保证下次遇到此样本时使得激励$x_j^{(i)} = w_j^{(i)}$能将其更多地判定为正类别，这也相当于增大其值大于单位阶跃函数阈值的概率，以使得此样本被判定为+1类  \n",
    "$$\\Delta{w_j} = (1^{(i)} - (-1^{(i)}))0.5^{(i)} = (2)0.5^{(i)}=1$$\n",
    "5  \n",
    "权重的更新与$x_j^{(i)}$的值成比例。例如，如果有另外一个样本$x_j^{(i)}=2$被错误分到了-1类别中，我们应更大幅度地移动决策边界，以保证下次遇到此样本时能正确分类  \n",
    "$$\\Delta{w_j} = (1^{(i)} - (-1^{(i)}))2^{(i)} = (2)2^{(i)}=4$$\n",
    "需要注意的是：感知器收敛的前提是两个类别必须是线性可分的，且学习速率足够小  \n",
    "如果两个类别无法通过一个线性决策边界进行划分，可以为模型在训练数据集上的学习迭代次数设置一个最大值，或者设置一个允许错误分类样本数量的阈值——否则，感知器训练算法将永远不停地更新权值  \n",
    "![2-3](../syn_pic/py_machine_learning/2-3.png)\n",
    "5  \n",
    "![2-4](../syn_pic/py_machine_learning/2-4.png)\n",
    "上图说明了感知器如何接受样本x的输入，并将其与权值w进行加权以计算净输入。进而净输入被传递到激励函数（在此为单位阶跃函数），然后生成值为+1或者-1的二值输出，并以其作为样本的预测类标  \n",
    "在学习阶段，此输出用来计算预测的误差并更新权重 5 \n",
    "# 2.2 使用Python实现感知器学习算法\n",
    "在上一节中，我们已经学习了罗森布拉特感知器的工作方式，现在使用Python来实现它，并且将其应用于第1章中提到的鸢尾花数据集中  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "'''\n",
    "python\tnumpy\tArray creation routines\tnp.zeros()\n",
    "python\tnumpy\tArray objects\tndarray.shape\n",
    "python\tpython\tBuilt-in Functions\trange()\n",
    "python\tpython\tBuilt-in Functions\tzip()\n",
    "python\tSequence Types — list, tuple, range\ts.append()\ts.append()\n",
    "python\tnumpy\tLinear algebra (numpy.linalg)\tnp.dot()\n",
    "python\tnumpy\tIndexing routines\tnp.where()\n",
    "'''\n",
    "class Perceptron(object):\n",
    "    \"\"\"感知器分类器\n",
    "    \n",
    "    参数\n",
    "    ----------\n",
    "    eta : float\n",
    "        学习速率 (介于 0.0 和 1.0)\n",
    "    n_iter : int\n",
    "        训练集的最大迭代次数\n",
    "\n",
    "    属性\n",
    "    ----------\n",
    "    w_ : 1d-array\n",
    "        权重\n",
    "    errors_ : list\n",
    "        被错误分类的样本数量列表\n",
    "    5 \n",
    "    \"\"\"\n",
    "    def __init__(self, eta = 0.01, n_iter = 10):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"使用训练集训练模型\n",
    "        \n",
    "        参数\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "            训练特征集，n_samples是样本量，n_features是特征数量\n",
    "    y : array-like, shape = [n_samples]\n",
    "        目标类标\n",
    "        \n",
    "    返回\n",
    "    ----------\n",
    "    self : object\n",
    "    5\n",
    "        \"\"\"\n",
    "        self.w_ = np.zeros(1 + X.shape[1]) \n",
    "        self.errors_ = []\n",
    "        \n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.eta * (target - self.predict(xi))\n",
    "                self.w_[1:] += update * xi\n",
    "                self.w_[0] += update\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"计算净输入（净输入函数）\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"单位阶跃函数（激励函数），返回类标 5\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在感知器实现过程中，我们实例化一个Perceptron对象时，给出了一个学习速率eta和在训练数据集上进行迭代的次数n_iter  \n",
    "通过fit方法，我们将self.w_中的权值初始化为一个零向量$R^{m+1}$，其中m是数据集中维度(特征)的数量，我们在此基础上增加一个0权重列(也就是激励函数的阈值，通过与激励函数的等式被移到式子中)  \n",
    "与python的列表类似，numpy也使用方括号([])对一维数组进行索引。对二维数组来说，第1个索引值对应数组的行，第2个索引值对应数组的列  \n",
    "初始化权重后，fit方法循环迭代数据集中的所有样本，并根据前面章节讨论过的感知器学习规则来更新权重  \n",
    "我们使用predict方法来计算类标，这个方法在fit方法中被调用，用于计算权重更新时的类标，在完成模型训练后，predict方法也用于预测未知数据的类标 5  \n",
    "此外，在每次迭代的过程中，我们收集每轮迭代中错误分类样本的数量，并将其存放于列表self.errors_中，以便后续对感知器在训练中表现的好坏做出判定  \n",
    "另外在net_input方法中使用的np.dot方法用于计算项链的点积$w^Tx$  \n",
    "除了使用numpy的a.dot(b)或np.dot(a,b)计算两个数组a和b的点积之外，还可以通过类似sum(i\\*j for i,j in zip(a,b))这样的经典python for循环结构来计算  \n",
    "numpy的优势在于其算术运算的向量化。我们可充分利用现代处理器单指令流多数据流架构的支持快速完成运算，而不是循环地对每个元素逐一进行计算  5\n",
    "## 基于鸢尾花数据集训练感知器模型  \n",
    "为了测试前面实现的感知器算法，我们从鸢尾花数据集中挑选了setosa和versicolor两种花的信息作为测试数据（二分类），当然也可以通过one-vs.all技术扩展到多类别的分类器应用中  \n",
    "另外出于可视化方面的原因，我们只考虑数据集中sepal-length和petal-length这两个特征  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tconda\tconda install\t==\n",
    "python\tsklearn\tdatasets\tload_iris()\n",
    "python\tsklearn\tdatasets\tload_iris() as_frame\n",
    "python\tsklearn\tdatasets\tload_iris() frame\n",
    "python\tpandas\tdataframe\td.tail()\n",
    "'''\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris(as_frame=True)\n",
    "df = iris.frame\n",
    "iris.frame.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**遇到问题-conda无法升级scikit-learn==0.23.1**  \n",
    "**背景**  \n",
    "\n",
    "由于load_iris() 需要scikit-learn版本在0.23以上\n",
    "所以我通过conda install升级scikit-learn\n",
    "结果提示兼容性问题：The environment is inconsistent, please check the package plan carefully 5  \n",
    "**计划**\n",
    "1. 搜集相关资料\n",
    "2. 分析相关资料\n",
    "3. 制订解决方案\n",
    "\n",
    "5  \n",
    "**方案**\n",
    "资料链接\n",
    "1. https://stackoverflow.com/questions/55527354/the-environment-is-inconsistent-please-check-the-package-plan-carefully \n",
    "建议：conda install anaconda\n",
    "2. https://www.cnpython.com/qa/124929\n",
    "建议：conda install anaconda\n",
    "3. https://blog.csdn.net/sinat_37267278/article/details/103305673\n",
    "同1\n",
    "4. https://blog.csdn.net/qq_34970603/article/details/106419573\n",
    "https://www.jianshu.com/p/19422477cc3c\n",
    "建议：conda install anaconda\n",
    "5. https://www.jianshu.com/p/19422477cc3c\n",
    "建议： conda install anaconda + conda update --all\n",
    "5  \n",
    "\n",
    "采用conda install anaconda  \n",
    "解决问题 5  \n",
    "python\tconda\tconda install\tERROR 5  \n",
    "——————  \n",
    "提取前100个类标，其中分别包含50个山鸢尾类标和50个变色鸢尾类标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tpandas\tdataframe\td.iloc[]\n",
    "python\tpandas\tseries\ts.values\n",
    "python\tpandas\tdataframe\td.values\n",
    "python\tmatplotlib\tPyplot function overview\tplt.scatter()\n",
    "python\tmatplotlib\tPyplot function overview\tplt.scatter() marker\n",
    "python\tmatplotlib\tPyplot function overview\tplt.scatter() c\n",
    "python\tmatplotlib\tcollections\tlabel\n",
    "python\tmatplotlib\tPyplot function overview\txlabel()\n",
    "python\tmatplotlib\tPyplot function overview\tylabel()\n",
    "python\tmatplotlib\tPyplot function overview\tlegend() loc\n",
    "python\tmatplotlib\tPyplot function overview\tfigure() figsize\n",
    "python\tmatplotlib\tPyplot function overview\txlim()\n",
    "python\tmatplotlib\tPyplot function overview\tylim()\n",
    "\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = df.iloc[:100,4].values\n",
    "y = np.where(y == 0, -1, 1)\n",
    "X = df.iloc[:100, [0,2]].values\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.scatter(X[:50, 0], X[:50, 1],\n",
    "           c = 'red', marker = 'o', label = 'setosa')\n",
    "plt.scatter(X[50:, 0], X[50:, 1],\n",
    "           c = 'blue', marker = 'x', label = 'versicolor')\n",
    "plt.xlabel('sepal length(cm)')\n",
    "plt.ylabel('petal length(cm)')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.ylim(0,6)\n",
    "plt.xlim(4,7.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以利用抽取出的鸢尾花数据子集来训练感知器了  \n",
    "同时，我们还将绘制每次迭代的错误分类数量的折线图，以检验算法是否收敛并找到可以分开两种类型鸢尾花的决策边界  \n",
    "**遇到问题-算法不收敛**  \n",
    "+ 方案1：剔除异常点\n",
    "+ 方案2：检查并调整算法\n",
    "\n",
    "由于该代码来源书籍，只有数据集与书本不一致，故采用方案1  \n",
    "5  \n",
    "**发现原因**  \n",
    "特征选择代码错误, 原书的数据集和现有数据集特征存放位置不同  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tmatplotlib\tPyplot function overview\tplot()\n",
    "python\tmatplotlib\tlines\tmarker\n",
    "'''\n",
    "ppn = Perceptron(eta = 0.1, n_iter = 10)\n",
    "ppn.fit(X, y)\n",
    "plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker = 'o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of misclassifications')\n",
    "plt.xlim(1,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上图所示，我们的分类器在第6次迭代后就已经收敛，并且具备对训练样本进行正确分类的能力。下面通过一个简单的函数来实现对二维数据集决策边界的可视化  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tnumpy\tArray manipulation routines\tunique()\n",
    "python\tBuilt-in Functions\tlen()\tlen()\n",
    "python\tmatplotlib\tcolors\tListedColormap()\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\tmin()\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\tmax()\n",
    "python\tnumpy\tArray creation routines\tarange() step\n",
    "python\tnumpy\tArray creation routines\tmeshgrid()\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\travel()\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\tndarray.T\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\treshape()\n",
    "python\tmatplotlib\tPyplot function overview\tcontourf()\n",
    "python\tBuilt-in Functions\tenumerate()\tenumerate()\n",
    "'''\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, resolution = 0.02):\n",
    "    # 设置散点生成器和色图\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    \n",
    "    # 绘制决策边界\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                          np.arange(x2_min, x2_max, resolution)) \n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha = 0.4, cmap = cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    \n",
    "    # 绘制样本分类\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x = X[y == cl, 0], y = X[y == cl, 1],\n",
    "                   alpha = 0.8, c = cmap(idx),\n",
    "                   marker = markers[idx], label = cl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "meshgrid详解 \n",
    "python\tnumpy\tArray creation routines\tlinspace() num\n",
    "\n",
    "nx, ny = (3, 2) # 可以另外测试 nx, ny = (4, 3)\n",
    "x = np.linspace(0, 1, nx)\n",
    "y = np.linspace(0, 1, ny)\n",
    "xv, yv = np.meshgrid(x, y)\n",
    "print(x)\n",
    "print('-'*6)\n",
    "print(y)\n",
    "print('-'*6)\n",
    "print(xv)\n",
    "print('-'*6)\n",
    "print(yv)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_decision_regions(X, y, classifier = ppn)\n",
    "plt.xlabel('sepal length(cm)')\n",
    "plt.ylabel('petal length(cm)')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frank Rosenblatt从数学上证明了：如果两个类别可以通过线性超平面进行划分，则感知器算法一定会收敛 5  \n",
    "# 2.3 自适应线性神经元及其学习的收敛性 \n",
    "在Frank Rosenblatt提出感知器算法几年之后， Bernard Widrow和他的博士生Tedd Hoff提出了Adaline算法， 这可以看作对之前算法的改进  \n",
    "Adaline算法阐明了代价函数的核心概念，并且对其作了最小化优化，这是理解逻辑斯蒂回归、支持向量机和后续章节涉及的回归模型等基于回归的高级机器学习分类算法的基础  \n",
    "基于Adaline规则的权重更新是通过一个连续的线性激励函数来完成的，而不像Rosenblatt感知器那样使用单位阶跃函数，这是二者的主要区别  \n",
    "Adeline算法中作用于净输入的激励函数$\\varphi (z)$是简单的恒等函数， 即$\\varphi(w^Tx)=w^Tx$  \n",
    "线性激励函数在更新权重的同时，我们使用量化器对类标进行预测，量化器与前面提到的单位阶跃函数类似，如下图所示 5  \n",
    "![2-5](../syn_pic/py_machine_learning/2-5.png)\n",
    "与前文介绍的感知器算法的示意图进行比较，可以看出区别在于：这里使用线性激励函数的连续型输出值，而不是二类别分类类标来计算模型的误差以及更新权重 5  \n",
    "## 2.3.1 通过梯度下降最小化代价函数\n",
    "机器学习中监督学习算法的一个核心组成在于：在学习阶段定义一个待优化的目标函数。这个目标函数通常是需要我们做最小化处理的代价函数  \n",
    "在Adaline中，我们可以将代价函数J定义为通过模型得到的输出与实际类标之间的误差平方和（SSE）  \n",
    "$$J(w) = \\frac{1}{2}\\Sigma_i{(y^{(i)}-\\phi(z^{(i)}))^2}$$\n",
    "在此，系数1/2只是出于方便的考虑， 它使我们更容易导出梯度， 具体将在下一段落中介绍  \n",
    "与单位阶跃函数相比，这种连续型激励函数的主要优点在于：其代价函数是可导的 5  \n",
    "此代价函数的另一个优点在于：它是一个凸函数；这样，我们可以通过简单、高效的梯度下降优化算法来得到权重，且能保证在对鸢尾花数据集中样本进行分类时代价函数最小  \n",
    "我们将梯度下降的原理形象地描述为下山，直到获得一个局部或者全局最小值  \n",
    "在每次迭代中，根据给定地学习速率和梯度地斜率，能够确定每次移动地步幅，我们按照步幅沿着梯度方向前进一步  \n",
    "![2-6](../syn_pic/py_machine_learning/2-6.png)\n",
    "通过梯度下降，我们可以基于代价函数J(w)沿梯度$\\nabla J(w)$方向做一次权重更新： 5  \n",
    "$$w: = w + \\Delta{w}$$\n",
    "在此，权重增量$\\nabla{w}$定义为负梯度与学习速率$\\eta$地乘积：  \n",
    "$$\\Delta{w} = - \\eta\\Delta{J(w)}$$\n",
    "为了计算代价函数地梯度，我们需要计算代价函数相对于每个权重$w_j$的偏导$\\frac{\\partial{J}}{\\partial{w_i}} = - \\Sigma_i{(y^{(i)}-\\phi{(z^{(i)})})x_j^{(i)}}$  \n",
    "这样我们就可以把对权重$w_j$的更新写作$\\Delta{w_j} = - \\eta \\frac{\\partial{J}}{\\partial{w_i}} = \\mu \\Sigma_i{(y^{(i)} - \\phi(z^{(i)}))x_j^{(i)}}$  \n",
    "5  \n",
    "由于所有权重被同时更新，Adaline学习规则可记为：$w = w + \\Delta{w}$  \n",
    "对于熟悉代数的读者来说，误差平方和代价函数对于第j个权重的偏导，可通过下列步骤得到：  \n",
    "![2-7](../syn_pic/py_machine_learning/2-7.png)\n",
    "虽然Adaline学习规则与感知器规则看起来类似，不过$\\varphi{(z^{(i)})}$中的$z^{(i)} = w^Tx^{(i)}$是实数，而不是整数类标  \n",
    "此外，权重的更新是基于训练集中所有样本完成的，这也是此方法被称作“批量”梯度下降的原因 5  \n",
    "## 2.3.2 使用Python实现自适应线性神经元  \n",
    "感知器规则与Adaline非常相似，我们将在前面实现的感知器代码的基础上修改fit方法，将其权重的更新改为通过梯度下降最小化代价函数来实现Adaline算法  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\tdot()\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\tsum()\n",
    "5  \n",
    "'''\n",
    "class AdalineGD(object):\n",
    "    \"\"\"自适应线性神经元分类器\n",
    "    \n",
    "    参数\n",
    "    ----------\n",
    "    eta : float\n",
    "        学习速率 (介于 0.0 和 1.0)\n",
    "    n_iter : int\n",
    "        训练集的最大迭代次数\n",
    "\n",
    "    属性\n",
    "    ----------\n",
    "    w_ : 1d-array\n",
    "        权重\n",
    "    errors_ : list\n",
    "        被错误分类的样本数量列表 \n",
    "    \"\"\"\n",
    "    def __init__(self, eta = 0.01, n_iter = 50):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"使用训练集训练模型\n",
    "        \n",
    "        参数\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "            训练特征集，n_samples是样本量，n_features是特征数量\n",
    "    y : array-like, shape = [n_samples]\n",
    "        目标类标\n",
    "        \n",
    "    返回\n",
    "    ----------\n",
    "    self : object\n",
    "        \"\"\"\n",
    "        self.w_ = np.zeros(1 + X.shape[1]) \n",
    "        self.cost_ = []\n",
    "        \n",
    "        for _ in range(self.n_iter):\n",
    "            output = self.net_input(X)\n",
    "            errors = (y - output)\n",
    "            self.w_[1:] += self.eta * X.T.dot(errors)\n",
    "            self.w_[0] += self.eta * errors.sum()\n",
    "            cost = (errors**2).sum() / 2.0\n",
    "            self.cost_.append(cost)\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"计算净输入（净输入函数）\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"计算线性激励值\n",
    "        \"\"\"\n",
    "        return self.net_input(X)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"单位阶跃函数（激励函数），返回类标\"\"\"\n",
    "        return np.where(self.activation(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与感知器中针对每一个样本做一次权重更新不同，我们基于训练数据集通过self.eta\\*errors.sum()来计算梯度的第0个位置的权重  \n",
    "通过self.eta\\*X.T.dot(errors)来计算1到m位置的权重，其中X.T.dot(errors)是特征矩阵与误差向量之间的乘积  \n",
    "我们设置一个列表self.cost_来存储代价函数的输出值以检查本轮训练后算法是否收敛  \n",
    "矩阵与向量的乘法计算类似于将矩阵中的每一行单独作为一个行向量与原列向量的点积计算。例如  \n",
    "![2-8](../syn_pic/py_machine_learning/2-8.png)\n",
    "5  \n",
    "在实践中，为优化收敛效果，常常需要通过实验来找到合适的学习速率$\\eta$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tmatplotlib\tPyplot function overview\tsubplots() figsize\n",
    "python\tmatplotlib\taxes\tplot()\n",
    "python\tnumpy\tMathematical functions\tlog10()\n",
    "python\tmatplotlib\taxes\tset_xlabel()\n",
    "python\tmatplotlib\taxes\tset_ylabel()\n",
    "python\tmatplotlib\taxes\tset_title()\n",
    "'''\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (8, 4))\n",
    "ada1 = AdalineGD(n_iter = 10, eta = 0.01).fit(X, y)\n",
    "ax[0].plot(range(1, len(ada1.cost_) + 1),\n",
    "          np.log10(ada1.cost_), marker = 'o')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('log(Sum-squared-error)')\n",
    "ax[0].set_title('Adaline - Learning rate 0.01')\n",
    "ada2 = AdalineGD(n_iter = 10, eta = 0.0001).fit(X, y)\n",
    "ax[1].plot(range(1, len(ada2.cost_) + 1),\n",
    "          ada2.cost_, marker = 'o')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Sum-squared-error')\n",
    "ax[1].set_title('Adaline - Learning rate 0.0001')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "左边的图像显示了学习速率过大可能会出现的问题——并没有使代价函数的值尽可能的低，反而因为算法跳过了全局最优解，导致误差随着迭代次数增加而增大  \n",
    "下图说明了我们如何通过更改特定的权重参数值来最小化代价函数J（左子图）。右子图则展示了，如果学习速率选择过大会发生说明情况  \n",
    "![2-9](../syn_pic/py_machine_learning/2-9.png)\n",
    "5 \n",
    "本书涉及的许多机器学习算法要求对特征值范围进行特征缩放，以优化算法的性能。梯度下降就是通过特征缩放而收益的众多算法之一  \n",
    "标准化的特征缩放方法可以使数据具备标准正态分布的特性。例如，为了对第j个特征的值进行标准化处理，只需要将其值与所有样本的平均值$\\mu_j$相减，并除以其标准差$\\sigma_j$  \n",
    "$$x_j'=\\frac{x_j-\\mu_j}{\\sigma_j}$$\n",
    "这里的$x_j$使包含训练样本n中第j个特征的所有值的向量  \n",
    "标准化可以简单地通过numpy的mean和std方法来完成： 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tArray creation routines\tcopy()\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\tmean()\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\tstd()\n",
    "'''\n",
    "X_std = np.copy(X)\n",
    "X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()\n",
    "X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在进行标准化操作后，我们以学习速率$\\eta = 0.01$再次对Adaline进行训练，看看它是否是收敛的 ：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdalineGD(n_iter = 15, eta = 0.01)\n",
    "ada.fit(X_std, y)\n",
    "plot_decision_regions(X_std, y, classifier = ada)\n",
    "plt.title('Adaline - Gradient Descent')\n",
    "plt.xlabel('sepal length [standardized]')\n",
    "plt.xlabel('petal length [standardized]')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()\n",
    "plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker = 'o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Sum-squared-error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上图所示， 以$\\eta = 0.01$为学习速率， Adaline算法在经过标准化特征处理的数据上训练可以收敛。虽然所有样本都被正确分类，但是其误差平方和（SSE）的值仍旧不为零 5  \n",
    "## 2.3.3 大规模机器学习与随机梯度下降\n",
    "假定现在有一个包含几百万条数据的巨大数据集，对许多机器学习应用来说，这个量是非同寻常的  \n",
    "由于向全局最优点移动的每一步都需要使用整个数据集来进行评估，因此这种情况下使用批量梯度下降的计算成本非常高  \n",
    "一个常用的替代批量梯度下降的优化算法是随机梯度下降，有时也称作迭代梯度下降或者在线梯度下降  \n",
    "与基于所有样本$x^{(i)}$的累积误差更新权重的策略不同：  \n",
    "$$\\Delta{w} = \\eta\\Sigma_i{(y^{(i)}-\\phi(z^{(i)}))}x^{(i)}$$\n",
    "5  \n",
    "我们每次使用一个训练样本渐进地更新权重：  \n",
    "$$\\eta(y^{(i)}-\\phi(z^{(i)}))x^{(i)}$$\n",
    "虽然随机梯度下降可看作对梯度下降地近似，但是由于权重更新更频繁，通常它能更快收敛  \n",
    "由于梯度的计算是基于单个训练样本来完成的，因此其误差曲面不及梯度下降的平滑，但这也使得随机梯度下降更容易跳出小范围的局部最优点  \n",
    "当实现随机梯度下降时，通常使用随着时间变化的自适应学习速率来替代固定学习速率$\\eta$，例如：$\\frac{c_1}{[迭代次数] + c_2}$，其中$c_1$和$c_2$均为常数 5  \n",
    "清注意，随机梯度下降不一定会得到全局最优解，但会趋近于它  \n",
    "随机梯度下降的另一个优势是我们可以将其用于在线学习。使用在线学习，系统可以及时地适应变化，同时如果考虑存储成本的话，也可以将训练数据在完成对模型的更新后丢弃  \n",
    "小批次学习是介于梯度下降和随机梯度下降之间的一种技术。可以将小批次学习理解为相对较小的训练数据子集上应用梯度下降——例如，一次使用50个样本  \n",
    "与梯度下降相比，由于权重的更新更加频繁，因此收敛速度更快  \n",
    "此外，小批次学习使得我们可以用向量化操作来替代for循环，从而进一步提高学习算法的计算效率 5  \n",
    "我们在Adaline学习规则的基础上将学习算法中的权重更新改为通过随机梯度下降来实现即可。现在把fit方法改为使用单个训练样本来更新权重  \n",
    "此外，我们增加了一个partial_fit方法，对于在线学习，此方法不会重置权重。为了检验算法在训练后是否收敛，我们将每次迭代后计算出的代价值作为训练样本的平均消耗   \n",
    "此外，我们还增加了一个shuffle训练数据选项，每次迭代前重排训练数据避免在优化代价函数阶段陷入循环。通过random_state参数，我们可以指定随机数种子以保持多次训练的一致性    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5 \n",
    "python\tBuilt-in Functions\tsum()\tsum()\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\travel()\n",
    "python\tnumpy\tRandom sampling (numpy.random)\tpermutation()\n",
    "'''\n",
    "from numpy.random import seed  \n",
    "class AdalineGD(object):\n",
    "    \"\"\"自适应线性神经元分类器 5\n",
    "    \n",
    "    参数\n",
    "    ----------\n",
    "    eta : float\n",
    "        学习速率 (介于 0.0 和 1.0)\n",
    "    n_iter : int\n",
    "        训练集的最大迭代次数\n",
    "\n",
    "    属性\n",
    "    ----------\n",
    "    w_ : 1d-array\n",
    "        权重\n",
    "    errors_ : list\n",
    "        被错误分类的样本数量列表 \n",
    "    shuffle : bool(default: True)\n",
    "        每次迭代随机重排训练数据\n",
    "        当值为True时则会避免代价函数陷入循环\n",
    "    random_state : int(default: None)\n",
    "        设置重排的随机种子和初始化权重  \n",
    "    \"\"\"\n",
    "    def __init__(self, eta = 0.01, n_iter = 10,\n",
    "                shuffle = True, random_state = None):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.w_initialized = False\n",
    "        self.shuffle = shuffle\n",
    "        if random_state:\n",
    "            seed(random_state)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"使用训练集训练模型 5\n",
    "        \n",
    "        参数\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_samples, n_features]\n",
    "            训练特征集，n_samples是样本量，n_features是特征数量\n",
    "        y : array-like, shape = [n_samples]\n",
    "        目标类标\n",
    "        \n",
    "        返回\n",
    "        ----------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        self._initialize_weights(X.shape[1])\n",
    "        self.cost_ = []\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)\n",
    "            cost = []\n",
    "            for xi, target in zip(X, y):\n",
    "                cost.append(self._update_weights(xi, target))\n",
    "            avg_cost = sum(cost)/len(y)\n",
    "            self.cost_.append(avg_cost)            \n",
    "        return self\n",
    "    \n",
    "    def partial_fit(self, X, y):\n",
    "        \"\"\"不重新初始化权重来训练数据集\"\"\"\n",
    "        if not self.w_initialized:\n",
    "            self._initialize_weights(X.shape[1])\n",
    "        if y.ravel().shape[0] > 1:\n",
    "            for xi, target in zip(X, y):\n",
    "                self._update_weights(xi, target)\n",
    "        else:\n",
    "            self._update_weights(X, y)\n",
    "        return self\n",
    "    \n",
    "    def _shuffle(self, X, y):\n",
    "        \"\"\"重排训练数据\"\"\"\n",
    "        r = np.random.permutation(len(y))\n",
    "        return X[r], y[r]\n",
    "    \n",
    "    def _initialize_weights(self, m):\n",
    "        '''初始化零权重向量'''\n",
    "        self.w_ = np.zeros(1 + m)\n",
    "        self.w_initialized = True\n",
    "        \n",
    "    def _update_weights(self, xi, target):\n",
    "        \"\"\"\n",
    "        使用自适应规则更新权重 5\n",
    "        \"\"\"\n",
    "        output = self.net_input(xi)\n",
    "        error = (target - output)\n",
    "        self.w_[1:] += self.eta * xi.dot(error)\n",
    "        self.w_[0] += self.eta * error\n",
    "        cost = 0.5 * error ** 2\n",
    "        return cost\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"计算净输入（净输入函数）\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"计算线性激励值\n",
    "        \"\"\"\n",
    "        return self.net_input(X)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"单位阶跃函数（激励函数），返回类标\"\"\"\n",
    "        return np.where(self.activation(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类器AdalineSGD中_shuffle方法的工作原理如下：通过numpy.random的permutation函数，我们生成一个包含0~100的不重复的随机序列  \n",
    "这些数字可以作为索引帮助打乱我们的特征矩阵和类标向量  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdalineSGD(n_iter = 15, eta = 0.01, random_state = 1)\n",
    "ada.fit(X_std, y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
