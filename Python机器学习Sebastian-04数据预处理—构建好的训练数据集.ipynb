{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本章简介\n",
    "机器学习算法最终学习结果的优劣取决于两个主要因素：数据的质量和数据中蕴含的有用信息的数量。因此，对其进行检验及预处理是至关重要的  \n",
    "在本章中，我们将讨论主要的数据预处理技术，这些技术可以高效地构建好的机器学习模型  \n",
    "本章将涵盖如下主题：  \n",
    "+ 数据集中缺失数据的删除和填充\n",
    "+ 数据格式化\n",
    "+ 模型构建中的特征选择 5  \n",
    "\n",
    "# 4.1 缺失数据的处理\n",
    "> 作者介绍了数据删除、数据填充两种缺失处理手段及其使用方法  \n",
    "\n",
    "通常，我们见到的缺失值是数据表中的空值，或者类似于NaN的占位符  \n",
    "如果我们忽略这些缺失值，将导致大部分的计算工具无法对原始数据进行处理，或者得到某些不可预知的结果  \n",
    "我们先通过一个文件构造一个简单的例子  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tio\tStringIO\tStringIO()\n",
    "\n",
    "'''\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "csv_data = '''A,B,C,D\n",
    "1.0,2.0,3.0,4.0\n",
    "5.0,6.0,,8.0\n",
    "10.0,11.0,12.0,'''\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以使用代码中的isnull返回一个布尔型的DataFrame值,通过sum方法，我们可以得到如下所示的每列中缺失值数量：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tpython\tdataframe\tisna()\n",
    "'''\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我们使用sklearn处理数据之前，可以通过dataframe的value属性来访问相关的numpy数组  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tpandas\tdataframe\tvalues\n",
    "'''\n",
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1 将存在缺失值的特征或样本删除\n",
    "> 介绍了如何使用dataframe中的dropna进行缺失值删除的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "删除数据集中包含缺失值的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "删除数据集中至少包含一个NaN值的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5  \n",
    "python\tpandas\tdataframe\tdropna() axis\n",
    "'''\n",
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只删除所有列为NaN的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tpandas\tdataframe\tdropna() how\n",
    "'''\n",
    "df.dropna(how='all') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只删除指定列含有NaN的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "python\tpandas\tdataframe\tdropna() subset\n",
    "'''\n",
    "df.dropna(subset=['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可能会删除过多的样本，导致分析结果可靠性不高。从另一方面讲，如果删除了过多的特征列，有可能会面临丢失有价值信息的风险，而这些信息是分类器用来区分类别所必需的 5  \n",
    "## 4.1.2 缺失数据填充\n",
    "> 作者介绍了最常用的处理确实数据的方法-插值技术  \n",
    "\n",
    "最常用的插值技术之一就是均值插补，即使用相应的特征均值来替换缺失值  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tsklearn\tImpute\tSimpleImputer()\n",
    "'''\n",
    "from sklearn.impute import SimpleImputer\n",
    "imr = SimpleImputer(strategy='mean')\n",
    "imr = imr.fit(df)\n",
    "imputed_data = imr.transform(df.values)\n",
    "imputed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.3 理解sklearn预估器的API\n",
    "> 本节介绍了sklearn预估器的API及其使用方法  \n",
    "\n",
    "Imputer类属于sklearn中的转换器类，主要用于数据的转换。这些类中常用的两个方法是fit和transform  \n",
    "其中，fit方法用于对数据集中的参数进行识别并构建相应的数据补齐模型，而transform方法则使用刚构建的数据补齐模型中其他数据的维度相同  \n",
    "![4-1](../syn_pic/py_machine_learning/4-1.png)\n",
    "我们在第3章中用到了分类器，它们在sklearn中属于预估器类别，其API的设计与转换器非常相似，预估器类包含一个predict方法，同时也包含一个transform方法 5  \n",
    "![4-2](../syn_pic/py_machine_learning/4-2.png)\n",
    "5  \n",
    "# 4.2 处理类别数据\n",
    "> 本章介绍了处理类别数据的方法：针对有序特征的映射，类标的编码以及针对标称特征独热编码  \n",
    "\n",
    "在真实数据集中，经常会出现一个或多个类别数据的特征列。在讨论类别数据时，又可以进一步将他们划分为标称特征和有序特征  \n",
    "有序特征为类别的值是可以排序的，而标称数据则不具备排序的特性 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "'''\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([\n",
    "    ['green', 'M', 10.1, 'class1'],\n",
    "    ['red', 'L', 13.5, 'class2'],\n",
    "    ['blue', 'XL', 15.3, 'class1']])\n",
    "\n",
    "df.columns = ['color', 'size', 'price', 'classlabel']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1 有序特征的映射\n",
    "> 本节介绍了如何手工定义有序特征的映射  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tpandas\tseries\tmap()\n",
    "'''\n",
    "size_mapping = {\n",
    "    'XL': 3,\n",
    "    'L': 2,\n",
    "    'M': 1}\n",
    "df['size'] = df['size'].map(size_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果在后续过程中需要将整数还原为有序字符串，可以简单地定义一个逆映射字典  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tdict\titems()\titems()\n",
    "'''\n",
    "inv_size_mapping = {v:k for k,v in size_mapping.items()}\n",
    "inv_size_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2 类标的编码\n",
    "> 本节介绍了给类标编码的实现方法  \n",
    "\n",
    "虽然sklearn大多数分类预估器都会在内部将类标转换为整数，但通过将类标转换为整数序列能够从技术角度避免某些问题的产生  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tBuilt-in Functions\tenumerate()\tenumerate()\n",
    "python\tnumpy\tArray manipulation routines\tunique()\n",
    "'''\n",
    "import numpy as np\n",
    "class_mapping = {label:idx for idx, label in \n",
    "                enumerate(np.unique(df['classlabel']))}\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们可以使用映射字典将类标转换为整数：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "'''\n",
    "df['classlabel'] = df['classlabel'].map(class_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过下列代码将映射字典中的键值对倒置，以将转换过的类标还原回原始的字符串表示：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_class_mapping = {v:k for k,v in class_mapping.items()}\n",
    "df['classlabel'] = df['classlabel'].map(inv_class_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，使用sklearn中的LabelEncoder类可以更加方便地完成对类标的整数编码工作：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tPreprocessing and Normalization\tLabelEncoder()\n",
    "python\tsklearn\tPreprocessing and Normalization\tfit_transform()\n",
    "'''\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "class_le = LabelEncoder()\n",
    "y = class_le.fit_transform(df['classlabel'].values)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以使用inverse_transform将整数类标还原为原始的字符串表示 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tsklearn\tPreprocessing and Normalization\tinverse_transform()\n",
    "'''\n",
    "class_le.inverse_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.3 标称特征上的独热编码\n",
    "> 介绍了针对标称特征的独热编码方法\n",
    "\n",
    "我们如果用LabelEncoder类处理数据集中标称数据格式的color列，就会犯处理类别数据时最常见的错误。颜色并没有特定的顺序，但是学习算法将假定green大于blue、red大于green  \n",
    "虽然算法最终还是能够生成有用的结果，然后这个结果可能不是最优的  \n",
    "解决此问题最常用的方法就是独热编码技术。理念就是创建一个新的虚拟特征，虚拟特征的每一列各代表标称数据的一个值  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tsklearn\tPreprocessing and Normalization\tOneHotEncoder()\n",
    "python\tscipy\tSparse matrices (scipy.sparse)\tcsr_matrix() toarray()\n",
    "'''\n",
    "X = df[['color']].values\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X)\n",
    "ohe.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下，当我们调用OneHotEncoder的transform方法时，它会返回一个稀疏矩阵。处于可视化的考虑，我们可以通过toarray方法将其转换为一个常规的NumPy数组  \n",
    "另外，我们可以通过pandas中的get_dummies方法，更加方便地实现独热编码技术中的虚拟特征  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tpandas\tGeneral functions\tget_dummies()\n",
    "'''\n",
    "pd.get_dummies(df[['price','color','size']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 将数据集划分为训练数据集和测试数据集\n",
    "> 本章介绍了如何用sklearn实现测试集和训练集的划分\n",
    "\n",
    "葡萄酒数据集是另一个开源数据集，可以通过UCI机器学习样本数据库获得，它包含178个葡萄酒样本，每个样本通过13个特征对其化学特征进行描述  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tpandas\tInput/Output\tread_csv() head\n",
    "python\tpandas\tdataframe\thead()\n",
    "'''\n",
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',\n",
    "                      header=None)\n",
    "df_wine.columns = ['Class label', 'Alcohol', \n",
    "                   'Malic acid', 'Ash',\n",
    "                  'Alcalinity of ash', 'Magnesium',\n",
    "                  'Total phenols', 'Flavanoids',\n",
    "                  'Nonflavanoid phenols',\n",
    "                  'Proanthocyanins',\n",
    "                  'Color intensity', 'Hue',\n",
    "                  'OD280/OD315 of diluted wines',\n",
    "                  'Proline']\n",
    "print('Class labels', np.unique(df_wine['Class label']))\n",
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将此数据集随机划分为测试数据集和训练数据集  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tModel Selection\ttrain_test_split()\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设定test_size=0.3, 可以将30%的样本划分到X_test和y_test，剩余的70%样本划分到X_train及y_train 5  \n",
    "在实际应用中，基于原始数据的大小，常用的划分比例是60:40、70:30、或者80:20。对于非常庞大的数据集，将训练集和测试集的比例按照90:10或者99:1进行划分也是常见且可以接受的  \n",
    "为了让模型获得最佳的性能，完成分类模型的测试后，通常在整个数据集上需再次对模型进行训练 5  \n",
    "# 4.4 将特征的值缩放到相同的区间\n",
    "> 本章介绍了特征缩放及其技术方法：\n",
    "\n",
    "**对大多数机器学习和优化算法而言，将特征的值缩放到相同的区间可以使其性能更佳**  \n",
    "## 特征缩放的必要性\n",
    "> 除决策树、随机森林算法之外，特征缩放的必要性的三大原因\n",
    "1. 优化算法-如梯度下降\n",
    "2. 正则化权重惩罚\n",
    "3. 保证对每个特征优化的倾向性不受量纲影响 5\n",
    "\n",
    "\n",
    "特征缩放的重要性可以通过一个简单的例子来描述。假定我们有两个特征：一个特征值的范围为1\\~10；另一个特征值的范围为1\\~10000。直观地说，算法将主要根据第二个特征上较大的误差进行权重的优化  \n",
    "目前，将不同的特征缩放到相同的区间有两个常用的方法：归一化和标准化。这两个词在不同的领域中使用较为宽松，由具体语境所确定  \n",
    "多数情况下，归一化指的是将特征的值缩放到区间[0,1],它是最小-最大缩放的一个特例。通过如下公式可以计算出一个新的样本$x^{(i)}$的值$x^{(i)}_{norm}$: 5   \n",
    "$$x^{(i)}_{norm}=\\frac{x^{(i)}-x_{min}}{x_{max}-x_{min}}$$\n",
    "其中，$x^{(i)}$是一个特定的样本，$x_{min}$和$x_{max}$是某特征列的最小值和最大值  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tPreprocessing and Normalization\tMinMaxScaler()\n",
    "'''\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "X_train_norm = mms.fit_transform(X_train)\n",
    "X_test_norm = mms.transform(X_test)\n",
    "X_test_norm[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在大部分机器学习算法中，标准化的方法更加实用  \n",
    "这是因为：许多线性模型，在对它们进行训练的最初阶段，即权重初始化阶段，可将其值设定0或是趋近于0的随机极小值  \n",
    "通过标准化，我们可以将特征列的均值设为0，方差为1，这更易于权重的更新 5  \n",
    "与最小-最大缩放将值限定在一个有限的区间不同，标准化方法保持了异常值所蕴含的有用信息，并且使得算法收到这些值的影响较小  \n",
    "标准化的过程可用如下方程表示：  \n",
    "$$x_{std}^{(i)}=\\frac{x^{(i)}-\\mu_x}{\\sigma_x}$$\n",
    "其中，$\\mu_x$和$\\sigma_x$分别为样本某个特征列的均值和标准差  \n",
    "在包含值为0~5的数据样本，采用标准化和归一化两种常用的技术进行特征缩放，其两者之间的区别如下表所示： 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train)\n",
    "X_test_std = stdsc.fit_transform(X_test)\n",
    "\n",
    "feature_compare = np.arange(0.0,6.0).reshape(-1,1)\n",
    "dfc = pd.DataFrame({'input' : np.arange(0.0,6.0),\n",
    "                'std' : stdsc.fit_transform(feature_compare).ravel(),\n",
    "                'mms' : mms.fit_transform(feature_compare).ravel()})\n",
    "dfc.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 选择有意义的特征\n",
    "> 本章介绍了两种常用减少过拟合问题的方法：正则化和特征选择降维  \n",
    "\n",
    "产生过拟合的原因是建立在给定训练数据集上的模型过于复杂，而常用的降低泛化误差的方案有：\n",
    "1. 收集更多的训练数据\n",
    "2. 通过正则化引入罚项\n",
    "3. 选择一个参数相对较少的简单模型\n",
    "4. 降低数据的维度   \n",
    "\n",
    "一般来说，收集更多的训练数据不太适用 5  \n",
    "## 4.5.1 适用L1正则化满足数据稀疏化\n",
    "> 本节介绍了L1正则化作用、效果及其使用方法\n",
    "\n",
    "L2正则化是通过对大的权重增加罚项以降低模型复杂度的一种方法  \n",
    "$$L2:\\|w\\|^2_2=\\sum^m_{j=1}{w^2_j}$$\n",
    "另外一种降低模型复杂度的方法则与L1正则化相关：  \n",
    "$$L1:\\|w\\|_1=\\sum^m_{j=1}{|w_j|}$$\n",
    "与L2正则化不同，L1正则化可生成稀疏的特征向量，且大多数的权值为0 5  \n",
    "当高维数据集中包含许多不相关的特征，尤其是不相关的特征数量大于样本数量时，权重的稀疏化处理能够发挥很大的作用。**L1正则化可以被视作一种特征选择技术**  \n",
    "为了更好地理解L1正则化如何对数据进行稀疏化，我们来看一下正则化的几何解释  \n",
    "![4-3](../syn_pic/py_machine_learning/4-3.png)\n",
    "我们可以将正则化看作是在代价函数中增加一个罚项，以对小的权重做出激励，换句话说，对大的权重做出了惩罚  \n",
    "通过正则化参数$\\lambda$来增加正则化项的强度，使得权重向0收缩，降低了模型对训练数据的依赖程度。我们使用下图来阐述L2罚项的概念 5  \n",
    "![4-4](../syn_pic/py_machine_learning/4-4.png)\n",
    "我们使用阴影表示的球代表二次的L2正则化项。在此，我们的权重系数不能超出正则化的区域-权重的组合不能落在阴影以外的区域  \n",
    "在罚项的约束下，最好的选择就是L2球与不含有罚项的代价函数等高线的切点。正则化参数$\\lambda$的值越大，含有罚项的代价函数增长得就越快，L2的球就会越小  \n",
    "这个例子的主要内容：目标是使得代价函数和罚项之和最小，这可以理解为通过增加偏差使得模型趋向于简单，以降低在训练数据不足的情况下拟合得到的模型的方差  \n",
    "由于L1的罚项是权重系数绝对值的和(L2罚项是二次的)，我们可以将其表示为菱形区域，如下图所示 5  \n",
    "![4-5](../syn_pic/py_machine_learning/4-5.png)\n",
    "代价函数等高线与L1菱形在$w_1=0$处相交。由于L1的边界不是圆滑的，这个交点有可能是最优的。椭圆形的代价函数边界与L1菱形边界的交点位于坐标轴上，这也就使得模型更加稀疏  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tLinear Models\tLogisticRegression() solver\n",
    "python\tsklearn\tLinear Models\tLogisticRegression() penalty\n",
    "python\tsklearn\tLinear Models\tLogisticRegression() score()\n",
    "'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(penalty='l1', C=0.1, solver='liblinear')\n",
    "lr.fit(X_train_std, y_train)\n",
    "print('Training accuracy:',lr.score(X_train_std, y_train))\n",
    "print('Training accuracy:',lr.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练和测试的精确度显示此模型未出现过拟合。通过lr.intercept_属性得到截距项    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tsklearn\tLinear Models\tLogisticRegression() intercept_\n",
    "'''\n",
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于我们在多类别数据集上试用了LogisticRegression对象，它默认使用一对多的方法。其中，第一个截距项为类别1相对于类别2、3的匹配结果、第二个截距项为类别2相对于类别1、3的匹配结果，第三个截距项则为类别3相对类别1、2的匹配结果  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一个向量包含13个权重值，通过与13维的葡萄酒数据集中的特征数据相乘来计算模型的净输入：  \n",
    "$$z=w_1x_1+\\dots+w_mx_m=\\sum^m_{j=0}{x_jw_j}=w^tx$$\n",
    "我们注意到，权重向量是稀疏的，这意味着只有少数几个非零项 5  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
