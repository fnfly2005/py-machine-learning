{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本章简介\n",
    "机器学习算法最终学习结果的优劣取决于两个主要因素：数据的质量和数据中蕴含的有用信息的数量。因此，对其进行检验及预处理是至关重要的  \n",
    "在本章中，我们将讨论主要的数据预处理技术，这些技术可以高效地构建好的机器学习模型  \n",
    "本章将涵盖如下主题：  \n",
    "+ 数据集中缺失数据的删除和填充\n",
    "+ 数据格式化\n",
    "+ 模型构建中的特征选择 5  \n",
    "\n",
    "# 4.1 缺失数据的处理\n",
    "> 作者介绍了数据删除、数据填充两种缺失处理手段及其使用方法  \n",
    "\n",
    "通常，我们见到的缺失值是数据表中的空值，或者类似于NaN的占位符  \n",
    "如果我们忽略这些缺失值，将导致大部分的计算工具无法对原始数据进行处理，或者得到某些不可预知的结果  \n",
    "我们先通过一个文件构造一个简单的例子  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tio\tStringIO\tStringIO()\n",
    "\n",
    "'''\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "csv_data = '''A,B,C,D\n",
    "1.0,2.0,3.0,4.0\n",
    "5.0,6.0,,8.0\n",
    "10.0,11.0,12.0,'''\n",
    "df = pd.read_csv(StringIO(csv_data))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以使用代码中的isnull返回一个布尔型的DataFrame值,通过sum方法，我们可以得到如下所示的每列中缺失值数量：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tpython\tdataframe\tisna()\n",
    "'''\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我们使用sklearn处理数据之前，可以通过dataframe的value属性来访问相关的numpy数组  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tpandas\tdataframe\tvalues\n",
    "'''\n",
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1 将存在缺失值的特征或样本删除\n",
    "> 介绍了如何使用dataframe中的dropna进行缺失值删除的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "删除数据集中包含缺失值的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "删除数据集中至少包含一个NaN值的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5  \n",
    "python\tpandas\tdataframe\tdropna() axis\n",
    "'''\n",
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只删除所有列为NaN的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tpandas\tdataframe\tdropna() how\n",
    "'''\n",
    "df.dropna(how='all') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只删除指定列含有NaN的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "python\tpandas\tdataframe\tdropna() subset\n",
    "'''\n",
    "df.dropna(subset=['C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可能会删除过多的样本，导致分析结果可靠性不高。从另一方面讲，如果删除了过多的特征列，有可能会面临丢失有价值信息的风险，而这些信息是分类器用来区分类别所必需的 5  \n",
    "## 4.1.2 缺失数据填充\n",
    "> 作者介绍了最常用的处理确实数据的方法-插值技术  \n",
    "\n",
    "最常用的插值技术之一就是均值插补，即使用相应的特征均值来替换缺失值  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tsklearn\tImpute\tSimpleImputer()\n",
    "'''\n",
    "from sklearn.impute import SimpleImputer\n",
    "imr = SimpleImputer(strategy='mean')\n",
    "imr = imr.fit(df)\n",
    "imputed_data = imr.transform(df.values)\n",
    "imputed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.3 理解sklearn预估器的API\n",
    "> 本节介绍了sklearn预估器的API及其使用方法  \n",
    "\n",
    "Imputer类属于sklearn中的转换器类，主要用于数据的转换。这些类中常用的两个方法是fit和transform  \n",
    "其中，fit方法用于对数据集中的参数进行识别并构建相应的数据补齐模型，而transform方法则使用刚构建的数据补齐模型中其他数据的维度相同  \n",
    "![4-1](../syn_pic/py_machine_learning/4-1.png)\n",
    "我们在第3章中用到了分类器，它们在sklearn中属于预估器类别，其API的设计与转换器非常相似，预估器类包含一个predict方法，同时也包含一个transform方法 5  \n",
    "![4-2](../syn_pic/py_machine_learning/4-2.png)\n",
    "5  \n",
    "# 4.2 处理类别数据\n",
    "> 本章介绍了处理类别数据的方法：针对有序特征的映射，类标的编码以及针对标称特征独热编码  \n",
    "\n",
    "在真实数据集中，经常会出现一个或多个类别数据的特征列。在讨论类别数据时，又可以进一步将他们划分为标称特征和有序特征  \n",
    "有序特征为类别的值是可以排序的，而标称数据则不具备排序的特性 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "'''\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([\n",
    "    ['green', 'M', 10.1, 'class1'],\n",
    "    ['red', 'L', 13.5, 'class2'],\n",
    "    ['blue', 'XL', 15.3, 'class1']])\n",
    "\n",
    "df.columns = ['color', 'size', 'price', 'classlabel']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1 有序特征的映射\n",
    "> 本节介绍了如何手工定义有序特征的映射  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tpandas\tseries\tmap()\n",
    "'''\n",
    "size_mapping = {\n",
    "    'XL': 3,\n",
    "    'L': 2,\n",
    "    'M': 1}\n",
    "df['size'] = df['size'].map(size_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果在后续过程中需要将整数还原为有序字符串，可以简单地定义一个逆映射字典  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tdict\titems()\titems()\n",
    "'''\n",
    "inv_size_mapping = {v:k for k,v in size_mapping.items()}\n",
    "inv_size_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2 类标的编码\n",
    "> 本节介绍了给类标编码的实现方法  \n",
    "\n",
    "虽然sklearn大多数分类预估器都会在内部将类标转换为整数，但通过将类标转换为整数序列能够从技术角度避免某些问题的产生  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tBuilt-in Functions\tenumerate()\tenumerate()\n",
    "python\tnumpy\tArray manipulation routines\tunique()\n",
    "'''\n",
    "import numpy as np\n",
    "class_mapping = {label:idx for idx, label in \n",
    "                enumerate(np.unique(df['classlabel']))}\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们可以使用映射字典将类标转换为整数：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "'''\n",
    "df['classlabel'] = df['classlabel'].map(class_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过下列代码将映射字典中的键值对倒置，以将转换过的类标还原回原始的字符串表示：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_class_mapping = {v:k for k,v in class_mapping.items()}\n",
    "df['classlabel'] = df['classlabel'].map(inv_class_mapping)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，使用sklearn中的LabelEncoder类可以更加方便地完成对类标的整数编码工作：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tPreprocessing and Normalization\tLabelEncoder()\n",
    "python\tsklearn\tPreprocessing and Normalization\tfit_transform()\n",
    "'''\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "class_le = LabelEncoder()\n",
    "y = class_le.fit_transform(df['classlabel'].values)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以使用inverse_transform将整数类标还原为原始的字符串表示 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tsklearn\tPreprocessing and Normalization\tinverse_transform()\n",
    "'''\n",
    "class_le.inverse_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.3 标称特征上的独热编码\n",
    "> 介绍了针对标称特征的独热编码方法\n",
    "\n",
    "我们如果用LabelEncoder类处理数据集中标称数据格式的color列，就会犯处理类别数据时最常见的错误。颜色并没有特定的顺序，但是学习算法将假定green大于blue、red大于green  \n",
    "虽然算法最终还是能够生成有用的结果，然后这个结果可能不是最优的  \n",
    "解决此问题最常用的方法就是独热编码技术。理念就是创建一个新的虚拟特征，虚拟特征的每一列各代表标称数据的一个值  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tsklearn\tPreprocessing and Normalization\tOneHotEncoder()\n",
    "python\tscipy\tSparse matrices (scipy.sparse)\tcsr_matrix() toarray()\n",
    "'''\n",
    "X = df[['color']].values\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(X)\n",
    "ohe.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下，当我们调用OneHotEncoder的transform方法时，它会返回一个稀疏矩阵。处于可视化的考虑，我们可以通过toarray方法将其转换为一个常规的NumPy数组  \n",
    "另外，我们可以通过pandas中的get_dummies方法，更加方便地实现独热编码技术中的虚拟特征  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tpandas\tGeneral functions\tget_dummies()\n",
    "'''\n",
    "pd.get_dummies(df[['price','color','size']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 将数据集划分为训练数据集和测试数据集\n",
    "> 本章介绍了如何用sklearn实现测试集和训练集的划分\n",
    "\n",
    "葡萄酒数据集是另一个开源数据集，可以通过UCI机器学习样本数据库获得，它包含178个葡萄酒样本，每个样本通过13个特征对其化学特征进行描述  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tpandas\tInput/Output\tread_csv() head\n",
    "python\tpandas\tdataframe\thead()\n",
    "'''\n",
    "df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data',\n",
    "                      header=None)\n",
    "df_wine.columns = ['Class label', 'Alcohol', \n",
    "                   'Malic acid', 'Ash',\n",
    "                  'Alcalinity of ash', 'Magnesium',\n",
    "                  'Total phenols', 'Flavanoids',\n",
    "                  'Nonflavanoid phenols',\n",
    "                  'Proanthocyanins',\n",
    "                  'Color intensity', 'Hue',\n",
    "                  'OD280/OD315 of diluted wines',\n",
    "                  'Proline']\n",
    "print('Class labels', np.unique(df_wine['Class label']))\n",
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将此数据集随机划分为测试数据集和训练数据集  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tModel Selection\ttrain_test_split()\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设定test_size=0.3, 可以将30%的样本划分到X_test和y_test，剩余的70%样本划分到X_train及y_train 5  \n",
    "在实际应用中，基于原始数据的大小，常用的划分比例是60:40、70:30、或者80:20。对于非常庞大的数据集，将训练集和测试集的比例按照90:10或者99:1进行划分也是常见且可以接受的  \n",
    "为了让模型获得最佳的性能，完成分类模型的测试后，通常在整个数据集上需再次对模型进行训练 5  \n",
    "# 4.4 将特征的值缩放到相同的区间\n",
    "> 本章介绍了特征缩放及其技术方法：\n",
    "\n",
    "**对大多数机器学习和优化算法而言，将特征的值缩放到相同的区间可以使其性能更佳**  \n",
    "## 特征缩放的必要性\n",
    "> 除决策树、随机森林算法之外，特征缩放的必要性的三大原因\n",
    "1. 优化算法-如梯度下降\n",
    "2. 正则化权重惩罚\n",
    "3. 保证对每个特征优化的倾向性不受量纲影响 5\n",
    "\n",
    "\n",
    "特征缩放的重要性可以通过一个简单的例子来描述。假定我们有两个特征：一个特征值的范围为1\\~10；另一个特征值的范围为1\\~10000。直观地说，算法将主要根据第二个特征上较大的误差进行权重的优化  \n",
    "目前，将不同的特征缩放到相同的区间有两个常用的方法：归一化和标准化。这两个词在不同的领域中使用较为宽松，由具体语境所确定  \n",
    "多数情况下，归一化指的是将特征的值缩放到区间[0,1],它是最小-最大缩放的一个特例。通过如下公式可以计算出一个新的样本$x^{(i)}$的值$x^{(i)}_{norm}$: 5   \n",
    "$$x^{(i)}_{norm}=\\frac{x^{(i)}-x_{min}}{x_{max}-x_{min}}$$\n",
    "其中，$x^{(i)}$是一个特定的样本，$x_{min}$和$x_{max}$是某特征列的最小值和最大值  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tPreprocessing and Normalization\tMinMaxScaler()\n",
    "'''\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mms = MinMaxScaler()\n",
    "X_train_norm = mms.fit_transform(X_train)\n",
    "X_test_norm = mms.transform(X_test)\n",
    "X_test_norm[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在大部分机器学习算法中，标准化的方法更加实用  \n",
    "这是因为：许多线性模型，在对它们进行训练的最初阶段，即权重初始化阶段，可将其值设定0或是趋近于0的随机极小值  \n",
    "通过标准化，我们可以将特征列的均值设为0，方差为1，这更易于权重的更新 5  \n",
    "与最小-最大缩放将值限定在一个有限的区间不同，标准化方法保持了异常值所蕴含的有用信息，并且使得算法收到这些值的影响较小  \n",
    "标准化的过程可用如下方程表示：  \n",
    "$$x_{std}^{(i)}=\\frac{x^{(i)}-\\mu_x}{\\sigma_x}$$\n",
    "其中，$\\mu_x$和$\\sigma_x$分别为样本某个特征列的均值和标准差  \n",
    "在包含值为0~5的数据样本，采用标准化和归一化两种常用的技术进行特征缩放，其两者之间的区别如下表所示： 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train)\n",
    "X_test_std = stdsc.fit_transform(X_test)\n",
    "\n",
    "feature_compare = np.arange(0.0,6.0).reshape(-1,1)\n",
    "dfc = pd.DataFrame({'input' : np.arange(0.0,6.0),\n",
    "                'std' : stdsc.fit_transform(feature_compare).ravel(),\n",
    "                'mms' : mms.fit_transform(feature_compare).ravel()})\n",
    "dfc.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 选择有意义的特征\n",
    "> 本章介绍了两种常用减少过拟合问题的方法：正则化和特征选择降维  \n",
    "\n",
    "产生过拟合的原因是建立在给定训练数据集上的模型过于复杂，而常用的降低泛化误差的方案有：\n",
    "1. 收集更多的训练数据\n",
    "2. 通过正则化引入罚项\n",
    "3. 选择一个参数相对较少的简单模型\n",
    "4. 降低数据的维度   \n",
    "\n",
    "一般来说，收集更多的训练数据不太适用 5  \n",
    "## 4.5.1 适用L1正则化满足数据稀疏化\n",
    "> 本节介绍了L1正则化作用、效果及其使用方法\n",
    "\n",
    "L2正则化是通过对大的权重增加罚项以降低模型复杂度的一种方法  \n",
    "$$L2:\\|w\\|^2_2=\\sum^m_{j=1}{w^2_j}$$\n",
    "另外一种降低模型复杂度的方法则与L1正则化相关：  \n",
    "$$L1:\\|w\\|_1=\\sum^m_{j=1}{|w_j|}$$\n",
    "与L2正则化不同，L1正则化可生成稀疏的特征向量，且大多数的权值为0 5  \n",
    "当高维数据集中包含许多不相关的特征，尤其是不相关的特征数量大于样本数量时，权重的稀疏化处理能够发挥很大的作用。**L1正则化可以被视作一种特征选择技术**  \n",
    "为了更好地理解L1正则化如何对数据进行稀疏化，我们来看一下正则化的几何解释  \n",
    "![4-3](../syn_pic/py_machine_learning/4-3.png)\n",
    "我们可以将正则化看作是在代价函数中增加一个罚项，以对小的权重做出激励，换句话说，对大的权重做出了惩罚  \n",
    "通过正则化参数$\\lambda$来增加正则化项的强度，使得权重向0收缩，降低了模型对训练数据的依赖程度。我们使用下图来阐述L2罚项的概念 5  \n",
    "![4-4](../syn_pic/py_machine_learning/4-4.png)\n",
    "我们使用阴影表示的球代表二次的L2正则化项。在此，我们的权重系数不能超出正则化的区域-权重的组合不能落在阴影以外的区域  \n",
    "在罚项的约束下，最好的选择就是L2球与不含有罚项的代价函数等高线的切点。正则化参数$\\lambda$的值越大，含有罚项的代价函数增长得就越快，L2的球就会越小  \n",
    "这个例子的主要内容：目标是使得代价函数和罚项之和最小，这可以理解为通过增加偏差使得模型趋向于简单，以降低在训练数据不足的情况下拟合得到的模型的方差  \n",
    "由于L1的罚项是权重系数绝对值的和(L2罚项是二次的)，我们可以将其表示为菱形区域，如下图所示 5  \n",
    "![4-5](../syn_pic/py_machine_learning/4-5.png)\n",
    "代价函数等高线与L1菱形在$w_1=0$处相交。由于L1的边界不是圆滑的，这个交点有可能是最优的。椭圆形的代价函数边界与L1菱形边界的交点位于坐标轴上，这也就使得模型更加稀疏  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tLinear Models\tLogisticRegression() solver\n",
    "python\tsklearn\tLinear Models\tLogisticRegression() penalty\n",
    "python\tsklearn\tLinear Models\tLogisticRegression() score()\n",
    "'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(penalty='l1', C=0.1, solver='liblinear')\n",
    "lr.fit(X_train_std, y_train)\n",
    "print('Training accuracy:',lr.score(X_train_std, y_train))\n",
    "print('Training accuracy:',lr.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练和测试的精确度显示此模型未出现过拟合。通过lr.intercept_属性得到截距项    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tsklearn\tLinear Models\tLogisticRegression() intercept_\n",
    "'''\n",
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于我们在多类别数据集上试用了LogisticRegression对象，它默认使用一对多的方法。其中，第一个截距项为类别1相对于类别2、3的匹配结果、第二个截距项为类别2相对于类别1、3的匹配结果，第三个截距项则为类别3相对类别1、2的匹配结果  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一个向量包含13个权重值，通过与13维的葡萄酒数据集中的特征数据相乘来计算模型的净输入：  \n",
    "$$z=w_1x_1+\\dots+w_mx_m=\\sum^m_{j=0}{x_jw_j}=w^tx$$\n",
    "我们注意到，权重向量是稀疏的，这意味着只有少数几个非零项 5  \n",
    "最后，我们来绘制以下正则化效果的图，展示了将正则化参数应用于多个特征上时所产生的不同的正则化效果：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tmatplotlib\tPyplot function overview\tsubplot()\n",
    "python\tsklearn\tLinear Models\tLogisticRegression() solver\n",
    "python\tsklearn\tLinear Models\tLogisticRegression() penalty\n",
    "python\tsklearn\tLinear Models\tLogisticRegression() C\n",
    "python\tsklearn\tLinear Models\tLogisticRegression() coef_\n",
    "python\tBuilt-in Functions\tzip()\tzip()\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\tshape\n",
    "python\tmatplotlib\tPyplot function overview\taxhline()\n",
    "python\tmatplotlib\tPyplot function overview\txscale()\n",
    "python\tmatplotlib\taxes\tlegend()\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "colors = ['blue', 'green', 'red', 'cyan',\n",
    "         'magenta', 'yellow', 'black',\n",
    "         'pink', 'lightgreen', 'lightblue',\n",
    "         'gray', 'indigo', 'orange']\n",
    "weights, params = [], []\n",
    "for c in np.arange(-4, 6):\n",
    "    lr = LogisticRegression(penalty='l1',\n",
    "                           C=10.0**c,\n",
    "                           random_state=0,\n",
    "                           solver='liblinear')\n",
    "    lr.fit(X_train_std, y_train)\n",
    "    weights.append(lr.coef_[1])\n",
    "    params.append(10.0**c)\n",
    "    \n",
    "weights = np.array(weights)\n",
    "for column, color in zip(range(weights.shape[1]), colors):\n",
    "    plt.plot(params, weights[:, column],\n",
    "            label=df_wine.columns[column+1],\n",
    "            color=color)\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=3)\n",
    "plt.xlim([10.0**(-5), 10.0**5])\n",
    "plt.ylabel('weight coefficient')\n",
    "plt.xlabel('C')\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='upper left')\n",
    "ax.legend(loc='upper center',\n",
    "         bbox_to_anchor=(1.38, 1.03),\n",
    "         ncol=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在强的正则化参数(C<0.1)作用下，罚项使得所有的特征权重都趋近于0 5  \n",
    "## 4.5.2 序列特征选择算法\n",
    "> 本节介绍了降维技术中的特征选择算法，并实现了一个序列特征选择算法SBS\n",
    "\n",
    "另一种降低过拟合问题的方法是通过特征选择进行降维，该方法对未经正则化处理的模型特别有效  \n",
    "降维技术主要分为两大类：特征选择和特征提取  \n",
    "序列特征选择算法是一种贪婪搜索算法，用于将原始的d维特征空间压缩到一个k维特征子空间，其中k\\<d  \n",
    "使用特征选择算法处于以下考虑：能够剔除不相关特征或噪声，自动选出与问题最相关的特征子集，从而提高计算效率或是降低模型的泛化误差 5  \n",
    "一个经典的序列特征选择算法是序列后向选择算法，其目的是在分类性能衰减最小的约束下，降低原始特征空间上的数据维度，以提高计算效率  \n",
    "SBS算法背后的理念非常简单：SBS依次从特征集合中删除某些特征，直到新的特征子空间包含指定数量的特征  \n",
    "为了确定每一步所需删除的特征，我们定义一个需要最小化的标准衡量函数J。该函数的计算准则是：比较判定分类器的性能在删除某个特定特征前后的差异  \n",
    "基于对SBS的定义，我们可以将算法总结为四个简单的步骤：  \n",
    "1. 设k=d进行算法初始化，其中d是特征空间$X_d$的维度 5\n",
    "2. 定义$x^-$为满足标准$x^-=argmaxJ(X_k-x)$最大化的特征，其中$x\\in{X_k}$\n",
    "3. 将特征$x^-$从特征集中删除：$X_{k-1}=X_k-x^1, k=k-1$\n",
    "4. 如果k等于目标特征数量，算法终止，否则跳转到第2步  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tBase classes and utility functions\tclone()\n",
    "python\tsklearn\tModel Selection\ttrain_test_split()\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\tshape\n",
    "python\tBuilt-in Functions\ttuple()\ttuple()\n",
    "python\titertools\tcombinations()\tcombinations()\n",
    "python\tnumpy\tSorting, searching, and counting\targmax()\n",
    "'''\n",
    "from sklearn.base import clone\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import accuracy_score\n",
    "class SBS():\n",
    "    def __init__(self, estimator, k_features,\n",
    "                scoring=accuracy_score,\n",
    "                test_size=0.25, random_state=1):\n",
    "        self.scoring = scoring\n",
    "        self.estimator = clone(estimator)\n",
    "        self.k_features = k_features\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_test_split(X, y, test_size=self.test_size,\n",
    "                            random_state=self.random_state)\n",
    "        dim = X_train.shape[1]\n",
    "        self.indices_ = tuple(range(dim))\n",
    "        self.subsets_ = [self.indices_]\n",
    "        score = self._calc_score(X_train, y_train,\n",
    "                                X_test, y_test, self.indices_)\n",
    "        self.scores_ = [score]\n",
    "        \n",
    "        while dim > self.k_features:\n",
    "            scores = []\n",
    "            subsets = []\n",
    "            \n",
    "            for p in combinations(self.indices_, r=dim-1):\n",
    "                score = self._calc_score(X_train, y_train,\n",
    "                                        X_test, y_test, p)\n",
    "                scores.append(score)\n",
    "                subsets.append(p)\n",
    "                \n",
    "            best = np.argmax(scores)\n",
    "            self.indices_ = subsets[best]\n",
    "            self.subsets_.append(self.indices_)\n",
    "            dim -= 1\n",
    "            self.scores_.append(scores[best])\n",
    "        self.k_score_ = self.scores_[-1]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[:, self.indices_]\n",
    "    \n",
    "    def _calc_score(self, X_train, y_train,\n",
    "                   X_test, y_test, indices):\n",
    "        self.estimator.fit(X_train[:, indices], y_train)\n",
    "        y_pred = self.estimator.predict(X_test[:, indices])\n",
    "        score = self.scoring(y_test, y_pred)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前面的实现中，我们使用参数k_features来指定需返回的特征数量。我们使用sklearn中的accuracy_score去衡量分类器的模型和评估其在特征空间上的性能 5  \n",
    "在fit方法的while循环中，通过itertools.combination函数创建的特征子集循环地进行评估和删减，直到特征子集达到预期维度  \n",
    "在每次迭代中，基于内部测试数据集X_test创建的self.scores_列表存储了最优特征子集的准确度分值  \n",
    "最终特征子集的列标被赋值给self.indices_，我们可以通过transform方法返回由选定特征列构成的一个新数组  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tNearest Neighbors\tKNeighborsClassifier()\n",
    "'''\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "sbs = SBS(knn, k_features=1)\n",
    "sbs.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的SBS算法在每一步中存储了最优特征子集的分值，下面绘制出KNN分类器的分类准确率，准确率数值是在验证数据集上计算得出的 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tmatplotlib\tPyplot function overview\tgrid()\n",
    "'''\n",
    "k_feat = [len(k) for k in sbs.subsets_]\n",
    "plt.plot(k_feat, sbs.scores_, marker='o')\n",
    "plt.ylim([0.7, 1.1])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of features')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
