{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tdatasets\tload_iris() feature_names\n",
    "python\tsklearn\tdatasets\tload_iris() target\n",
    "python\tsklearn\tdatasets\tload_iris() data\n",
    "'''\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target\n",
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tmodel_selection\ttrain_test_split()\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler() \n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tLinear Models\tPerceptron() max_iter\n",
    "python\tsklearn\tLinear Models\tPerceptron() eta0\n",
    "'''\n",
    "from sklearn.linear_model import Perceptron\n",
    "ppn = Perceptron(max_iter = 40, eta0 = 0.1, random_state = 0)\n",
    "ppn.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tstr\tformat()\tformat() d\n",
    "'''\n",
    "y_pred = ppn.predict(X_test_std)\n",
    "print('Misclassified samples: {:d}'.format((y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tsklearn\tClassification metrics\taccuracy_score()\n",
    "'''\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tArray manipulation routines\tunique()\n",
    "python\tmatplotlib\tcolors\tListedColormap()\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\tmin()\n",
    "python\tnumpy\tThe N-dimensional array (ndarray)\tmax()\n",
    "python\tnumpy\tArray creation routines\tmeshgrid()\n",
    "python\tmatplotlib\tPyplot function overview\tcontourf()\n",
    "python\tmatplotlib\tPyplot function overview\tscatter() alpha\n",
    "python\tmatplotlib\tcollections\tlabel\n",
    "'''\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_decision_regions(X, y, classifier,\n",
    "                          test_idx = None, resolution = 0.02):\n",
    "    # 设置散点生成器和色图\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    \n",
    "    # 绘制决策边界\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                          np.arange(x2_min, x2_max, resolution)) \n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha = 0.4, cmap = cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "    \n",
    "    # 绘制样本分类\n",
    "    X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x = X[y == cl, 0], y = X[y == cl, 1],\n",
    "                   alpha = 0.8, color = cmap(idx),\n",
    "                   marker = markers[idx], label = cl)\n",
    "\n",
    "    # 高亮测试样本\n",
    "    if test_idx:\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "        plt.scatter(X_test[:, 0], X_test[:, 1], c = 'yellow',\n",
    "                   alpha = 0.5, linewidth = 1, marker = 'o',\n",
    "                   s = 55, label = 'test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tArray manipulation routines\tvstack()\n",
    "python\tnumpy\tArray manipulation routines\thstack()\n",
    "python\tmatplotlib\tPyplot function overview\tlegend() loc\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "X_combined_std = np.vstack((X_train_std, X_test_std))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "plot_decision_regions(X = X_combined_std,\n",
    "                     y = y_combined,\n",
    "                     classifier = ppn,\n",
    "                     test_idx = range(105,150))\n",
    "plt.xlabel('petal length [standardized]')\n",
    "plt.ylabel('petal length [standardized]')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "python\tnumpy\tMathematical functions\texp()\n",
    "python\tmatplotlib\tPyplot function overview\taxvline()\n",
    "python\tmatplotlib\tlines\tcolor\n",
    "python\tmatplotlib\tPyplot function overview\taxhspan()\n",
    "python\tmatplotlib\tpatches\tPatch() facecolor\n",
    "python\tmatplotlib\tpatches\tPatch() alpha\n",
    "python\tmatplotlib\tpatches\tPatch() linestyle\n",
    "python\tmatplotlib\tPyplot function overview\taxhline()\n",
    "python\tmatplotlib\tlines\tlinestyle\n",
    "\n",
    "'''\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "z = np.arange(-7, 7, 0.1)\n",
    "phi_z = sigmoid(z)\n",
    "plt.plot(z, phi_z)\n",
    "plt.axvline(0.0, color='k')\n",
    "plt.axhspan(0.0, 1.0, facecolor='1.0', alpha=1.0, linestyle='dotted')\n",
    "plt.axhline(y=0.5, linestyle='dotted', color='black')\n",
    "plt.axhline(y=0, linestyle='dotted', color='black')\n",
    "plt.axhline(y=1.0, linestyle='dotted', color='black')\n",
    "plt.yticks([0.0, 0.5, 1.0])\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('$\\phi (z)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(C=1000.0, random_state=0)\n",
    "lr.fit(X_train_std, y_train)\n",
    "plot_decision_regions(X_combined_std,\n",
    "                     y_combined, classifier=lr,\n",
    "                     test_idx=range(105,150))\n",
    "plt.xlabel('petal length [standardized]')\n",
    "plt.ylabel('petal width [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在进入下一步之前，先计算一下sigmoid函数的偏导：  \n",
    "![3-3](../syn_pic/py_machine_learning/3-3.png)  \n",
    "我们的目标时求得能够使对数似然函数最大化的权重值，在此需按如下公式更新所有权重：  \n",
    "$$w_j:=w_j+\\eta\\sum_{i=1}^n{(y^{(i)}-\\phi(z^{(i)}))x^{(i)}}$$\n",
    "由于我们是同时更新所有的权重的，因此可以将更新规则记为： 5  \n",
    "$$w:=w+\\Delta{w}$$\n",
    "其中，$\\Delta{w}$定义为：  \n",
    "$$\\Delta{w}=\\eta\\nabla{l(w)}$$\n",
    "由于最大化对数似然函数等价于最小化前面定义的代价函数J，因此可以将梯度下降的更新规则定义为：  \n",
    "$$\\Delta{w_j}=-\\eta\\frac{\\partial{J}}{\\partial{w_j}}=\\eta\\sum_{i=1}^n{(y^{(i)}-\\phi{(z^{(i)})x^{(i)}})}$$\n",
    "$$w:=w+\\Delta{w},\\Delta{w}=-\\eta\\nabla{J(w)}$$\n",
    "5  \n",
    "这等价于第2章中的梯度下降规则 5  \n",
    "## 3.3.4 通过正则化解决过拟合问题  \n",
    "过拟合是机器学习中的常见问题，它是指模型在训练数据集上表现良好，但是用于未知数据时性能不佳  \n",
    "如果一个模型出现了过拟合问题，我们也说此模型有高方差，这有可能是因为使用了相关数据中过多的参数，从而使得模型变得国于复杂。同样，模型也可能面临欠拟合（高偏差）问题  \n",
    "![3-4](../syn_pic/py_machine_learning/3-4.png)  \n",
    "如果我们多次重复训练一个模型，如使用训练数据集中不同的子集，方差可以用来衡量模型对特定样本实例预测的一致性。可以说模型对训练数据中的随机性是敏感的  \n",
    "相反，当我们在不同的训练数据集上多次重建模型时，偏差可以从总体上衡量预测值与实际值之间的差异;偏差并不是由样本的随机性导致的，它衡量的是系统误差 5  \n",
    "偏差-方差权衡就是通过正则化调整模型的复杂度。正则化是解决共线性（特征间高度相关）的一个很有用的方法，它可以过滤掉数据中的噪音，并最终防止过拟合  \n",
    "### 问题：为什么正则化可以防止过拟合？\n",
    "> 由于过拟合本质是过多的特征被启用导致的，导致模型泛化性变差，所以防止过拟合要降低特征的数量，可以通过使w个数减少，问题就变成让W向量中项的个数最小化，方法就是让w变成或趋近于0，因为向量中0元素对应的x是没有任何权重的 5  \n",
    "### 问题：什么是L2正则化？\n",
    "> L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的正则项$\\|W\\|^2$最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，这里是有很大的区别的哦 5  \n",
    "\n",
    "正则化背后的概念是引入额外的信息（偏差）来对极端参数权重做出惩罚。最常用的正则化形式被称为L2正则化，有时也称作L2收缩或权重衰减，写作  \n",
    "$$\\frac{\\lambda}{2}\\|{w}\\|^2=\\frac{\\lambda}{2}\\sum_{j=1}^mw_j^2$$\n",
    "其中，$\\lambda$为正则化系数  \n",
    "**特征缩放之所以重要，其中一个原因就是正则化。为了使得正则化起作用，需要确保所有特征保持统一** 5  \n",
    "使用正则化方法时，我们只需在逻辑斯蒂回归的代价函数中加入正则化项，以降低回归系数带来的副作用：  \n",
    "$$J(w)=\\left\\{\\sum_{i=1}^n{-log(\\phi(z^{(i)}))+(1-y^{(i)})(-log(1-\\phi(z^{(i)})))}\\right\\}+\\frac{\\lambda}{2}\\|w\\|^2$$  \n",
    "通过正则化系数，保持权值较小时，我们就可以控制模型与训练数据的拟合程度  \n",
    "前面用到了sklearn中的LogisticRegression类，其中的参数C来自支持向量机中的约定，它时正则化系数的倒数：  \n",
    "$$C=\\frac{1}{\\lambda}$$\n",
    "5  \n",
    "由此，我们可以将逻辑斯蒂回归中经过正则化的代价函数写作  \n",
    "$$J(w)=C\\left\\{\\sum_{i=1}^n{-log(\\phi(z^{(i)}))+(1-y^{(i)})(-log(1-\\phi(z^{(i)})))}\\right\\}+\\frac{\\lambda}{2}\\|w\\|^2$$\n",
    "因此，减小正则化参数倒数C的值相当于增加正则化的强度，这可以通过绘制对两个权重系数进行L2正则化后的图像予以展示  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "5\n",
    "python\tConcrete exceptions\tValueError\tIntegers to negative integer powers are not allowed\n",
    "python\tsklearn\tLinear Models\tLogisticRegression() C\n",
    "python\tsklearn\tLinear Models\tLogisticRegression() coef_\n",
    "python\tmatplotlib\tlines\tlabel\n",
    "python\tmatplotlib\tPyplot function overview\txscale()\n",
    "'''\n",
    "weights, params = [], []\n",
    "for c in np.arange(-5, 5):\n",
    "    lr = LogisticRegression(C=10.0**c, random_state=0) # 原书写着C=10**c 会引起ValueError\n",
    "    lr.fit(X_train_std, y_train)\n",
    "    weights.append(lr.coef_[1])\n",
    "    params.append(10.0**c) # 原书写着C=10**c 会引起ValueError    \n",
    "weights = np.array(weights)\n",
    "plt.plot(params, weights[:, 0],\n",
    "        label='petal length')\n",
    "plt.plot(params, weights[:, 1], linestyle='--',\n",
    "        label='petal width')\n",
    "plt.ylabel('weight coefficient')\n",
    "plt.xlabel('C')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过结果图像可以看到，如果我们减小参数C的值，也就是增加正则化项的强度，可以导致权重系数逐渐收缩 5  \n",
    "# 3.4 使用支持向量机最大化分类间隔\n",
    "另一种性能强大且广泛应用的学习算法是支持向量机，它可以看作对感知器的扩展。在SVM中，我们的优化目标是最大化分类间隔  \n",
    "此处间隔是指两个分离的超平面间（决策边界）的距离，而最靠近超平面的训练样本称作支持向量，如下图所示：  \n",
    "![3-5](../syn_pic/py_machine_learning/3-5.png)  \n",
    "5  \n",
    "## 3.4.1 对分类间隔最大化的直观认识\n",
    "决策边界间具有较大的间隔意味着模型具有较小的泛化误差，而较小的间隔则意味着模型可能过拟合  \n",
    "为了对间隔最大化有个直观认识，我们仔细观察一下两条平行的决策边界，我们分别称其为正负超平面，可表示为：  \n",
    "$$w_0+w^Tx_{pos}=1$$(1)\n",
    "$$w_0+w^Tx_{neg}=-1$$(2)  \n",
    "如果我们将等式（1）（2）相减，可以得到： 5  \n",
    "$$\\Rightarrow{w^T(x_{pos}-x_{neg})=2}$$ \n",
    "我们可以通过向量w的长度来对其进行规范化，做如下定义：  \n",
    "$$\\|w\\|=\\sqrt{\\sum_{j=1}^m{w_j^2}}$$\n",
    "由此可以得到如下等式：  \n",
    "$$\\frac{w^T(x_{pos}-x_{neg})}{\\|w\\|}=\\frac{2}{\\|w\\|}$$\n",
    "5  \n",
    "上述等式的左侧可以解释为正、负超平面间的距离，也就是我们要最大化的间隔  \n",
    "在样本正确划分的前提下，最大化分类间隔也就是使$\\frac{2}{\\|w\\|}$最大化，这也是SVM目标函数，记为：  \n",
    "$$w_0+w^Tx^{(i)}\\ge1若y^{(i)}=1$$\n",
    "$$w_0+w^Tx^{(i)}\\lt-1若y^{(i)}=-1$$\n",
    "这两个方程可以解释为：所有的负样本都落在负超平面一侧，而所有的正样本则在超平面划分出的区域中。它们可以写成更紧凑的形式： 5  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
